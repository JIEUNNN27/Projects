{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8dbc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: wheel in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from lightgbm) (0.38.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from lightgbm) (1.10.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from lightgbm) (1.24.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from lightgbm) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Implying --no-binary=:all: due to the presence of --build-option / --global-option / --install-option. Consider using --config-settings for more flexibility.\n",
      "DEPRECATION: --install-option is deprecated because it forces pip to use the 'setup.py install' command which is itself deprecated. pip 23.1 will enforce this behaviour change. A possible replacement is to use --config-settings. Discussion can be found at https://github.com/pypa/pip/issues/11358\n",
      "DEPRECATION: --no-binary currently disables reading from the cache of locally built wheels. In the future --no-binary will not influence the wheel cache. pip 23.1 will enforce this behaviour change. A possible replacement is to use the --no-cache-dir option. You can use the flag --use-feature=no-binary-enable-wheel-cache to test the upcoming behaviour. Discussion can be found at https://github.com/pypa/pip/issues/11453\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm --install-option=--gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f69af2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bayesian-optimization\n",
      "  Downloading bayesian_optimization-1.4.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from bayesian-optimization) (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from bayesian-optimization) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from bayesian-optimization) (1.10.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from bayesian-optimization) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
      "Installing collected packages: bayesian-optimization\n",
      "Successfully installed bayesian-optimization-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e200717c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.1.0-py3-none-any.whl (365 kB)\n",
      "     ---------------------------------------- 0.0/365.3 kB ? eta -:--:--\n",
      "     ------------------------------- ------ 307.2/365.3 kB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 365.3/365.3 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (1.4.46)\n",
      "Collecting cmaes>=0.9.1\n",
      "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (23.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (1.10.2)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from optuna) (1.24.2)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.2)\n",
      "Installing collected packages: colorlog, cmaes, optuna\n",
      "Successfully installed cmaes-0.9.1 colorlog-6.7.0 optuna-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "781f73f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:15:41,666]\u001b[0m A new study created in memory with name: no-name-f4029f90-e4ff-475a-b5f4-ee0ba38be2db\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1941034719.py:34: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:15:45,585]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 150, 'learning_rate': 0.08986751129609181, 'num_leaves': 33, 'max_depth': 18, 'min_child_samples': 16} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [9480, 75840]').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1941034719.py\", line 42, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 299, in train\n",
      "    evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3271, in eval_valid\n",
      "    return [item for i in range(1, self.__num_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3272, in <listcomp>\n",
      "    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3809, in __inner_eval\n",
      "    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 176, in __call__\n",
      "    return self.func(labels, preds)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1941034719.py\", line 42, in <lambda>\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1146, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1287, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1573, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1374, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 86, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [9480, 75840]\n",
      "\u001b[33m[W 2023-03-19 03:15:45,585]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9480, 75840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    298\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[1;32m--> 299\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3271\u001b[0m, in \u001b[0;36mBooster.eval_valid\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[0;32m   3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i, feval)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[1;32m-> 3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_valid_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:176\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n\u001b[0;32m    174\u001b[0m argc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(labels, preds, dataset\u001b[38;5;241m.\u001b[39mget_weight())\n",
      "Cell \u001b[1;32mIn[19], line 42\u001b[0m, in \u001b[0;36mobjective.<locals>.<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9480, 75840]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the data\n",
    "X = vectorizer.fit_transform(train_df['text'])\n",
    "y = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X = csr_matrix(X, dtype='float64')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X, y, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c407aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:16:47,874]\u001b[0m A new study created in memory with name: no-name-5e4f753f-9047-4a06-b437-310a2c024777\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1782866026.py:34: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:16:50,956]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 103, 'learning_rate': 0.06818874101705212, 'num_leaves': 18, 'max_depth': 4, 'min_child_samples': 25} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [9480, 75840]').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1782866026.py\", line 42, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 299, in train\n",
      "    evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3271, in eval_valid\n",
      "    return [item for i in range(1, self.__num_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3272, in <listcomp>\n",
      "    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3809, in __inner_eval\n",
      "    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 176, in __call__\n",
      "    return self.func(labels, preds)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1782866026.py\", line 42, in <lambda>\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1146, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1287, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1573, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1374, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 86, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [9480, 75840]\n",
      "\u001b[33m[W 2023-03-19 03:16:50,957]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9480, 75840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    298\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[1;32m--> 299\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3271\u001b[0m, in \u001b[0;36mBooster.eval_valid\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[0;32m   3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i, feval)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[1;32m-> 3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_valid_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:176\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n\u001b[0;32m    174\u001b[0m argc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(labels, preds, dataset\u001b[38;5;241m.\u001b[39mget_weight())\n",
      "Cell \u001b[1;32mIn[20], line 42\u001b[0m, in \u001b[0;36mobjective.<locals>.<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9480, 75840]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the data\n",
    "X = vectorizer.fit_transform(train_df['text'])\n",
    "y = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X = X.astype('float64')\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X, y, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = X_test.astype('float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9cb948aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:17:59,369]\u001b[0m A new study created in memory with name: no-name-c644a06f-f227-488b-be4c-e082c9755d65\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1338981769.py:31: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:18:06,414]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 61, 'learning_rate': 0.2147845245978224, 'num_leaves': 40, 'max_depth': 12, 'min_child_samples': 6} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [9480, 75840]').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1338981769.py\", line 39, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 299, in train\n",
      "    evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3271, in eval_valid\n",
      "    return [item for i in range(1, self.__num_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3272, in <listcomp>\n",
      "    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3809, in __inner_eval\n",
      "    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 176, in __call__\n",
      "    return self.func(labels, preds)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1338981769.py\", line 39, in <lambda>\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1146, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1287, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1573, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1374, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 86, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [9480, 75840]\n",
      "\u001b[33m[W 2023-03-19 03:18:06,414]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9480, 75840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[21], line 39\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     27\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     36\u001b[0m }\n\u001b[0;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     42\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    298\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[1;32m--> 299\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3271\u001b[0m, in \u001b[0;36mBooster.eval_valid\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[0;32m   3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i, feval)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[1;32m-> 3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_valid_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:176\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n\u001b[0;32m    174\u001b[0m argc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(labels, preds, dataset\u001b[38;5;241m.\u001b[39mget_weight())\n",
      "Cell \u001b[1;32mIn[21], line 39\u001b[0m, in \u001b[0;36mobjective.<locals>.<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     27\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     36\u001b[0m }\n\u001b[0;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 39\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     41\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     42\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9480, 75840]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X_train = csr_matrix(X_train, dtype='float64')\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ceaaf202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:19:24,055]\u001b[0m A new study created in memory with name: no-name-375f75d7-89d1-496d-ab60-0632bf2353ca\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py:34: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:19:26,710]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 61, 'learning_rate': 0.008399674874692332, 'num_leaves': 53, 'max_depth': 4, 'min_child_samples': 36} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [9480, 75840]').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py\", line 42, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 299, in train\n",
      "    evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3271, in eval_valid\n",
      "    return [item for i in range(1, self.__num_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3272, in <listcomp>\n",
      "    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3809, in __inner_eval\n",
      "    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 176, in __call__\n",
      "    return self.func(labels, preds)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py\", line 42, in <lambda>\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1146, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1287, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1573, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1374, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 86, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [9480, 75840]\n",
      "\u001b[33m[W 2023-03-19 03:19:26,710]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9480, 75840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    298\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[1;32m--> 299\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3271\u001b[0m, in \u001b[0;36mBooster.eval_valid\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[0;32m   3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i, feval)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[1;32m-> 3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_valid_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:176\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n\u001b[0;32m    174\u001b[0m argc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(labels, preds, dataset\u001b[38;5;241m.\u001b[39mget_weight())\n",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m, in \u001b[0;36mobjective.<locals>.<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9480, 75840]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X_train = csr_matrix(X_train, dtype='float64')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the training data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1263c883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:20:21,338]\u001b[0m A new study created in memory with name: no-name-fdf86c1d-b392-4d61-9387-b57e1f14e0be\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py:34: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:20:24,110]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 86, 'learning_rate': 0.4585305911611971, 'num_leaves': 24, 'max_depth': 11, 'min_child_samples': 34} because of the following error: ValueError('Found input variables with inconsistent numbers of samples: [9480, 75840]').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py\", line 42, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 299, in train\n",
      "    evaluation_result_list.extend(booster.eval_valid(feval))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3271, in eval_valid\n",
      "    return [item for i in range(1, self.__num_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3272, in <listcomp>\n",
      "    for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 3809, in __inner_eval\n",
      "    feval_ret = eval_function(self.__inner_predict(data_idx), cur_data)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 176, in __call__\n",
      "    return self.func(labels, preds)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\2500633042.py\", line 42, in <lambda>\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1146, in f1_score\n",
      "    return fbeta_score(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1287, in fbeta_score\n",
      "    _, _, f, _ = precision_recall_fscore_support(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1573, in precision_recall_fscore_support\n",
      "    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 1374, in _check_set_wise_labels\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py\", line 86, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 397, in check_consistent_length\n",
      "    raise ValueError(\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [9480, 75840]\n",
      "\u001b[33m[W 2023-03-19 03:20:24,110]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [9480, 75840]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:299\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    298\u001b[0m         evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(booster\u001b[38;5;241m.\u001b[39meval_train(feval))\n\u001b[1;32m--> 299\u001b[0m     evaluation_result_list\u001b[38;5;241m.\u001b[39mextend(\u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_after_iter:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3271\u001b[0m, in \u001b[0;36mBooster.eval_valid\u001b[1;34m(self, feval)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[0;32m   3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__inner_eval(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_valid_sets[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], i, feval)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3272\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_valid\u001b[39m(\u001b[38;5;28mself\u001b[39m, feval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3241\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate for validation data.\u001b[39;00m\n\u001b[0;32m   3242\u001b[0m \n\u001b[0;32m   3243\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;124;03m        List with evaluation results.\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   3271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [item \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)\n\u001b[1;32m-> 3272\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname_valid_sets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeval\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:3809\u001b[0m, in \u001b[0;36mBooster.__inner_eval\u001b[1;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[0;32m   3807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 3809\u001b[0m feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43meval_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__inner_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m   3811\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m eval_name, val, is_higher_better \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:176\u001b[0m, in \u001b[0;36m_EvalFunctionWrapper.__call__\u001b[1;34m(self, preds, dataset)\u001b[0m\n\u001b[0;32m    174\u001b[0m argc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(signature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m argc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(labels, preds, dataset\u001b[38;5;241m.\u001b[39mget_weight())\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mobjective.<locals>.<lambda>\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, eval_set\u001b[38;5;241m=\u001b[39m[(X_valid, y_valid)], early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1146\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf1_score\u001b[39m(\n\u001b[0;32m   1012\u001b[0m     y_true,\n\u001b[0;32m   1013\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1020\u001b[0m ):\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1287\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfbeta_score\u001b[39m(\n\u001b[0;32m   1159\u001b[0m     y_true,\n\u001b[0;32m   1160\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1168\u001b[0m ):\n\u001b[0;32m   1169\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \n\u001b[0;32m   1171\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\sklearn\\utils\\validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    395\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    399\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    400\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [9480, 75840]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X_train = csr_matrix(X_train, dtype='float64')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the training data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric=lambda y_true, y_pred: ('macro_f1', f1_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "977428c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:21:22,276]\u001b[0m A new study created in memory with name: no-name-dd9537fc-87b4-4d5f-b697-1ec138ed1b73\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\873452568.py:34: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\u001b[33m[W 2023-03-19 03:21:22,292]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 128, 'learning_rate': 0.047211826500795355, 'num_leaves': 11, 'max_depth': 4, 'min_child_samples': 30} because of the following error: LightGBMError(\"Multiclass objective and metrics don't match\").\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\873452568.py\", line 42, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric='macro_f1')\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 967, in fit\n",
      "    super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py\", line 748, in fit\n",
      "    self._Booster = train(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 1534, in _lazy_init\n",
      "    self.__init_from_csr(data, params_str, ref_dataset)\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 1728, in __init_from_csr\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromCSR(\n",
      "  File \"C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Multiclass objective and metrics don't match\n",
      "\u001b[33m[W 2023-03-19 03:21:22,294]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Multiclass objective and metrics don't match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Optimize the objective function using Optuna\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters found\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[24], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     30\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmulticlassova\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     39\u001b[0m }\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_valid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_valid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_valid)\n\u001b[0;32m     45\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(y_valid, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:967\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m             valid_sets[i] \u001b[38;5;241m=\u001b[39m (valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y))\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:748\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    745\u001b[0m evals_result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    746\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m--> 748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evals_result:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    273\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   2599\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   2600\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2601\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   2602\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2603\u001b[0m     )\n\u001b[0;32m   2604\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 2605\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 1815\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1816\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1817\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1818\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   1821\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:1534\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1528\u001b[0m     _safe_call(_LIB\u001b[38;5;241m.\u001b[39mLGBM_DatasetCreateFromFile(\n\u001b[0;32m   1529\u001b[0m         c_str(\u001b[38;5;28mstr\u001b[39m(data)),\n\u001b[0;32m   1530\u001b[0m         c_str(params_str),\n\u001b[0;32m   1531\u001b[0m         ref_dataset,\n\u001b[0;32m   1532\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mbyref(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle)))\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsr_matrix):\n\u001b[1;32m-> 1534\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__init_from_csr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, scipy\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mcsc_matrix):\n\u001b[0;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__init_from_csc(data, params_str, ref_dataset)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:1728\u001b[0m, in \u001b[0;36mDataset.__init_from_csr\u001b[1;34m(self, csr, params_str, ref_dataset)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m csr\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m MAX_INT32\n\u001b[0;32m   1726\u001b[0m csr_indices \u001b[38;5;241m=\u001b[39m csr\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1728\u001b[0m \u001b[43m_safe_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_DatasetCreateFromCSR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_indptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_indptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcsr_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOINTER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptr_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtype_ptr_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\basic.py:125\u001b[0m, in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check the return value from C API call.\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \n\u001b[0;32m    119\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;124;03m    The return value from C API calls.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(_LIB\u001b[38;5;241m.\u001b[39mLGBM_GetLastError()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Multiclass objective and metrics don't match"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X_train = csr_matrix(X_train, dtype='float64')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclassova',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric='macro_f1')\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the training data using the best hyperparameters\n",
    "params = study.best_params\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric='macro_f1')\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f5e2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:22:51,752]\u001b[0m A new study created in memory with name: no-name-70ffd7f3-0df2-48b5-b198-e6095ae124b6\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.626706\n",
      "[20]\tvalid_0's multi_logloss: 0.468982\n",
      "[30]\tvalid_0's multi_logloss: 0.406242\n",
      "[40]\tvalid_0's multi_logloss: 0.372455\n",
      "[50]\tvalid_0's multi_logloss: 0.351069\n",
      "[60]\tvalid_0's multi_logloss: 0.336393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:23:02,396]\u001b[0m Trial 0 finished with value: 0.8470799542764375 and parameters: {'n_estimators': 60, 'learning_rate': 0.13529608024496062, 'num_leaves': 37, 'max_depth': 11, 'min_child_samples': 46}. Best is trial 0 with value: 0.8470799542764375.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.46805\n",
      "[20]\tvalid_0's multi_logloss: 1.35844\n",
      "[30]\tvalid_0's multi_logloss: 1.26932\n",
      "[40]\tvalid_0's multi_logloss: 1.19515\n",
      "[50]\tvalid_0's multi_logloss: 1.13113\n",
      "[60]\tvalid_0's multi_logloss: 1.07531\n",
      "[70]\tvalid_0's multi_logloss: 1.02595\n",
      "[80]\tvalid_0's multi_logloss: 0.981821\n",
      "[90]\tvalid_0's multi_logloss: 0.942492\n",
      "[100]\tvalid_0's multi_logloss: 0.907201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:23:23,760]\u001b[0m Trial 1 finished with value: 0.6965905875254157 and parameters: {'n_estimators': 103, 'learning_rate': 0.006188938212008328, 'num_leaves': 19, 'max_depth': 18, 'min_child_samples': 13}. Best is trial 0 with value: 0.8470799542764375.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.830582\n",
      "[20]\tvalid_0's multi_logloss: 0.616112\n",
      "[30]\tvalid_0's multi_logloss: 0.513748\n",
      "[40]\tvalid_0's multi_logloss: 0.457184\n",
      "[50]\tvalid_0's multi_logloss: 0.420269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:23:32,995]\u001b[0m Trial 2 finished with value: 0.8424277410026804 and parameters: {'n_estimators': 59, 'learning_rate': 0.07221991818265969, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 48}. Best is trial 0 with value: 0.8470799542764375.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.810331\n",
      "[20]\tvalid_0's multi_logloss: 0.57548\n",
      "[30]\tvalid_0's multi_logloss: 0.462671\n",
      "[40]\tvalid_0's multi_logloss: 0.400408\n",
      "[50]\tvalid_0's multi_logloss: 0.362224\n",
      "[60]\tvalid_0's multi_logloss: 0.337084\n",
      "[70]\tvalid_0's multi_logloss: 0.319789\n",
      "[80]\tvalid_0's multi_logloss: 0.307634\n",
      "[90]\tvalid_0's multi_logloss: 0.298851\n",
      "[100]\tvalid_0's multi_logloss: 0.292304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:24:03,515]\u001b[0m Trial 3 finished with value: 0.848080773855524 and parameters: {'n_estimators': 105, 'learning_rate': 0.061476166145602876, 'num_leaves': 46, 'max_depth': 0, 'min_child_samples': 19}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.682106\n",
      "[20]\tvalid_0's multi_logloss: 0.512608\n",
      "[30]\tvalid_0's multi_logloss: 0.440115\n",
      "[40]\tvalid_0's multi_logloss: 0.399145\n",
      "[50]\tvalid_0's multi_logloss: 0.372734\n",
      "[60]\tvalid_0's multi_logloss: 0.35454\n",
      "[70]\tvalid_0's multi_logloss: 0.340838\n",
      "[80]\tvalid_0's multi_logloss: 0.330857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:24:26,444]\u001b[0m Trial 4 finished with value: 0.8322879848168484 and parameters: {'n_estimators': 86, 'learning_rate': 0.12964201092040972, 'num_leaves': 21, 'max_depth': 17, 'min_child_samples': 9}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.894929\n",
      "[20]\tvalid_0's multi_logloss: 1.3759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:24:32,372]\u001b[0m Trial 5 finished with value: 0.6794240857940145 and parameters: {'n_estimators': 113, 'learning_rate': 0.23546251569171703, 'num_leaves': 29, 'max_depth': 4, 'min_child_samples': 15}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.679855\n",
      "[20]\tvalid_0's multi_logloss: 0.673772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:24:42,282]\u001b[0m Trial 6 finished with value: 0.7279651765528228 and parameters: {'n_estimators': 130, 'learning_rate': 0.2013597909216689, 'num_leaves': 41, 'max_depth': 12, 'min_child_samples': 13}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.39718\n",
      "[20]\tvalid_0's multi_logloss: 1.24927\n",
      "[30]\tvalid_0's multi_logloss: 1.13585\n",
      "[40]\tvalid_0's multi_logloss: 1.04435\n",
      "[50]\tvalid_0's multi_logloss: 0.968931\n",
      "[60]\tvalid_0's multi_logloss: 0.905733\n",
      "[70]\tvalid_0's multi_logloss: 0.851221\n",
      "[80]\tvalid_0's multi_logloss: 0.804144\n",
      "[90]\tvalid_0's multi_logloss: 0.763014\n",
      "[100]\tvalid_0's multi_logloss: 0.727117\n",
      "[110]\tvalid_0's multi_logloss: 0.695511\n",
      "[120]\tvalid_0's multi_logloss: 0.667309\n",
      "[130]\tvalid_0's multi_logloss: 0.641832\n",
      "[140]\tvalid_0's multi_logloss: 0.618262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:25:05,122]\u001b[0m Trial 7 finished with value: 0.7889736208058615 and parameters: {'n_estimators': 146, 'learning_rate': 0.009370322615428223, 'num_leaves': 33, 'max_depth': 15, 'min_child_samples': 42}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.34344\n",
      "[20]\tvalid_0's multi_logloss: 1.17683\n",
      "[30]\tvalid_0's multi_logloss: 1.05437\n",
      "[40]\tvalid_0's multi_logloss: 0.959019\n",
      "[50]\tvalid_0's multi_logloss: 0.882777\n",
      "[60]\tvalid_0's multi_logloss: 0.81984\n",
      "[70]\tvalid_0's multi_logloss: 0.767895\n",
      "[80]\tvalid_0's multi_logloss: 0.72374\n",
      "[90]\tvalid_0's multi_logloss: 0.685729\n",
      "[100]\tvalid_0's multi_logloss: 0.652782\n",
      "[110]\tvalid_0's multi_logloss: 0.62458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:25:37,273]\u001b[0m Trial 8 finished with value: 0.7905756515413355 and parameters: {'n_estimators': 116, 'learning_rate': 0.012665706948600355, 'num_leaves': 28, 'max_depth': 12, 'min_child_samples': 9}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.38522\n",
      "[20]\tvalid_0's multi_logloss: 1.26778\n",
      "[30]\tvalid_0's multi_logloss: 1.18918\n",
      "[40]\tvalid_0's multi_logloss: 1.13046\n",
      "[50]\tvalid_0's multi_logloss: 1.08405\n",
      "[60]\tvalid_0's multi_logloss: 1.04664\n",
      "[70]\tvalid_0's multi_logloss: 1.01508\n",
      "[80]\tvalid_0's multi_logloss: 0.987927\n",
      "[90]\tvalid_0's multi_logloss: 0.964053\n",
      "[100]\tvalid_0's multi_logloss: 0.943044\n",
      "[110]\tvalid_0's multi_logloss: 0.923669\n",
      "[120]\tvalid_0's multi_logloss: 0.906296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:25:43,797]\u001b[0m Trial 9 finished with value: 0.6397074966412044 and parameters: {'n_estimators': 125, 'learning_rate': 0.03525581114054807, 'num_leaves': 30, 'max_depth': 1, 'min_child_samples': 25}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.65985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:25:47,932]\u001b[0m Trial 10 finished with value: 0.6200083483046472 and parameters: {'n_estimators': 83, 'learning_rate': 0.44152065180911454, 'num_leaves': 55, 'max_depth': 6, 'min_child_samples': 29}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.00282\n",
      "[20]\tvalid_0's multi_logloss: 0.77844\n",
      "[30]\tvalid_0's multi_logloss: 0.655915\n",
      "[40]\tvalid_0's multi_logloss: 0.582065\n",
      "[50]\tvalid_0's multi_logloss: 0.531195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:26:00,092]\u001b[0m Trial 11 finished with value: 0.8152758768793307 and parameters: {'n_estimators': 56, 'learning_rate': 0.04709370517028059, 'num_leaves': 45, 'max_depth': 8, 'min_child_samples': 31}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.636471\n",
      "[20]\tvalid_0's multi_logloss: 0.444313\n",
      "[30]\tvalid_0's multi_logloss: 0.367389\n",
      "[40]\tvalid_0's multi_logloss: 0.329037\n",
      "[50]\tvalid_0's multi_logloss: 0.308848\n",
      "[60]\tvalid_0's multi_logloss: 0.296418\n",
      "[70]\tvalid_0's multi_logloss: 0.289072\n",
      "[80]\tvalid_0's multi_logloss: 0.28446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:26:23,856]\u001b[0m Trial 12 finished with value: 0.847163881472795 and parameters: {'n_estimators': 82, 'learning_rate': 0.09989118268039904, 'num_leaves': 49, 'max_depth': -1, 'min_child_samples': 21}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.14805\n",
      "[20]\tvalid_0's multi_logloss: 0.914181\n",
      "[30]\tvalid_0's multi_logloss: 0.764737\n",
      "[40]\tvalid_0's multi_logloss: 0.662124\n",
      "[50]\tvalid_0's multi_logloss: 0.586988\n",
      "[60]\tvalid_0's multi_logloss: 0.529887\n",
      "[70]\tvalid_0's multi_logloss: 0.486512\n",
      "[80]\tvalid_0's multi_logloss: 0.451818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:26:51,367]\u001b[0m Trial 13 finished with value: 0.8183575789716859 and parameters: {'n_estimators': 86, 'learning_rate': 0.022935477178703687, 'num_leaves': 51, 'max_depth': -1, 'min_child_samples': 21}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.11656\n",
      "[20]\tvalid_0's multi_logloss: 0.953384\n",
      "[30]\tvalid_0's multi_logloss: 0.859503\n",
      "[40]\tvalid_0's multi_logloss: 0.795657\n",
      "[50]\tvalid_0's multi_logloss: 0.748833\n",
      "[60]\tvalid_0's multi_logloss: 0.712541\n",
      "[70]\tvalid_0's multi_logloss: 0.68325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:26:56,853]\u001b[0m Trial 14 finished with value: 0.7624565241778943 and parameters: {'n_estimators': 79, 'learning_rate': 0.07385131946616684, 'num_leaves': 48, 'max_depth': 2, 'min_child_samples': 35}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.11974\n",
      "[20]\tvalid_0's multi_logloss: 0.880263\n",
      "[30]\tvalid_0's multi_logloss: 0.729711\n",
      "[40]\tvalid_0's multi_logloss: 0.627943\n",
      "[50]\tvalid_0's multi_logloss: 0.554808\n",
      "[60]\tvalid_0's multi_logloss: 0.500218\n",
      "[70]\tvalid_0's multi_logloss: 0.457548\n",
      "[80]\tvalid_0's multi_logloss: 0.424976\n",
      "[90]\tvalid_0's multi_logloss: 0.398776\n",
      "[100]\tvalid_0's multi_logloss: 0.378424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:27:27,278]\u001b[0m Trial 15 finished with value: 0.8263164902423193 and parameters: {'n_estimators': 100, 'learning_rate': 0.02452267543306801, 'num_leaves': 59, 'max_depth': -1, 'min_child_samples': 21}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.971037\n",
      "[20]\tvalid_0's multi_logloss: 0.782313\n",
      "[30]\tvalid_0's multi_logloss: 0.684721\n",
      "[40]\tvalid_0's multi_logloss: 0.622063\n",
      "[50]\tvalid_0's multi_logloss: 0.580518\n",
      "[60]\tvalid_0's multi_logloss: 0.54722\n",
      "[70]\tvalid_0's multi_logloss: 0.521521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:27:36,655]\u001b[0m Trial 16 finished with value: 0.7942603786965896 and parameters: {'n_estimators': 70, 'learning_rate': 0.0760765039377301, 'num_leaves': 11, 'max_depth': 4, 'min_child_samples': 20}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.23434\n",
      "[20]\tvalid_0's multi_logloss: 1.07595\n",
      "[30]\tvalid_0's multi_logloss: 0.979137\n",
      "[40]\tvalid_0's multi_logloss: 0.911604\n",
      "[50]\tvalid_0's multi_logloss: 0.860672\n",
      "[60]\tvalid_0's multi_logloss: 0.819387\n",
      "[70]\tvalid_0's multi_logloss: 0.785763\n",
      "[80]\tvalid_0's multi_logloss: 0.757394\n",
      "[90]\tvalid_0's multi_logloss: 0.73375\n",
      "[100]\tvalid_0's multi_logloss: 0.712928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:27:42,795]\u001b[0m Trial 17 finished with value: 0.7390677772951872 and parameters: {'n_estimators': 100, 'learning_rate': 0.044516398109681554, 'num_leaves': 41, 'max_depth': 2, 'min_child_samples': 36}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.802641\n",
      "[20]\tvalid_0's multi_logloss: 0.616507\n",
      "[30]\tvalid_0's multi_logloss: 0.532811\n",
      "[40]\tvalid_0's multi_logloss: 0.48488\n",
      "[50]\tvalid_0's multi_logloss: 0.450921\n",
      "[60]\tvalid_0's multi_logloss: 0.428087\n",
      "[70]\tvalid_0's multi_logloss: 0.410081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:28:08,816]\u001b[0m Trial 18 finished with value: 0.8123859907127324 and parameters: {'n_estimators': 71, 'learning_rate': 0.10250541383121924, 'num_leaves': 53, 'max_depth': 7, 'min_child_samples': 6}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.27282\n",
      "[20]\tvalid_0's multi_logloss: 1.09788\n",
      "[30]\tvalid_0's multi_logloss: 0.981528\n",
      "[40]\tvalid_0's multi_logloss: 0.896417\n",
      "[50]\tvalid_0's multi_logloss: 0.833301\n",
      "[60]\tvalid_0's multi_logloss: 0.784417\n",
      "[70]\tvalid_0's multi_logloss: 0.745037\n",
      "[80]\tvalid_0's multi_logloss: 0.712731\n",
      "[90]\tvalid_0's multi_logloss: 0.684857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:28:19,670]\u001b[0m Trial 19 finished with value: 0.7709725926239579 and parameters: {'n_estimators': 93, 'learning_rate': 0.02470004505277792, 'num_leaves': 59, 'max_depth': 4, 'min_child_samples': 25}. Best is trial 3 with value: 0.848080773855524.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.777937\n",
      "[20]\tvalid_0's multi_logloss: 0.550844\n",
      "[30]\tvalid_0's multi_logloss: 0.444221\n",
      "[40]\tvalid_0's multi_logloss: 0.387197\n",
      "[50]\tvalid_0's multi_logloss: 0.352344\n",
      "[60]\tvalid_0's multi_logloss: 0.329174\n",
      "[70]\tvalid_0's multi_logloss: 0.313598\n",
      "[80]\tvalid_0's multi_logloss: 0.302899\n",
      "[90]\tvalid_0's multi_logloss: 0.29466\n",
      "[100]\tvalid_0's multi_logloss: 0.288505\n",
      "[110]\tvalid_0's multi_logloss: 0.28372\n",
      "[120]\tvalid_0's multi_logloss: 0.280665\n",
      "[130]\tvalid_0's multi_logloss: 0.277462\n",
      "[140]\tvalid_0's multi_logloss: 0.275211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:29:00,588]\u001b[0m Trial 20 finished with value: 0.8629610968444204 and parameters: {'n_estimators': 148, 'learning_rate': 0.06807179262756581, 'num_leaves': 45, 'max_depth': -1, 'min_child_samples': 17}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.806612\n",
      "[20]\tvalid_0's multi_logloss: 0.573255\n",
      "[30]\tvalid_0's multi_logloss: 0.46088\n",
      "[40]\tvalid_0's multi_logloss: 0.40019\n",
      "[50]\tvalid_0's multi_logloss: 0.36248\n",
      "[60]\tvalid_0's multi_logloss: 0.337497\n",
      "[70]\tvalid_0's multi_logloss: 0.320208\n",
      "[80]\tvalid_0's multi_logloss: 0.308638\n",
      "[90]\tvalid_0's multi_logloss: 0.299108\n",
      "[100]\tvalid_0's multi_logloss: 0.293242\n",
      "[110]\tvalid_0's multi_logloss: 0.288259\n",
      "[120]\tvalid_0's multi_logloss: 0.284603\n",
      "[130]\tvalid_0's multi_logloss: 0.281749\n",
      "[140]\tvalid_0's multi_logloss: 0.279321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:29:39,856]\u001b[0m Trial 21 finished with value: 0.8440146849342175 and parameters: {'n_estimators': 144, 'learning_rate': 0.06287183922922729, 'num_leaves': 45, 'max_depth': -1, 'min_child_samples': 18}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.19118\n",
      "[20]\tvalid_0's multi_logloss: 1.04981\n",
      "[30]\tvalid_0's multi_logloss: 0.968127\n",
      "[40]\tvalid_0's multi_logloss: 0.911163\n",
      "[50]\tvalid_0's multi_logloss: 0.867278\n",
      "[60]\tvalid_0's multi_logloss: 0.833304\n",
      "[70]\tvalid_0's multi_logloss: 0.803238\n",
      "[80]\tvalid_0's multi_logloss: 0.778805\n",
      "[90]\tvalid_0's multi_logloss: 0.756681\n",
      "[100]\tvalid_0's multi_logloss: 0.737838\n",
      "[110]\tvalid_0's multi_logloss: 0.721397\n",
      "[120]\tvalid_0's multi_logloss: 0.705862\n",
      "[130]\tvalid_0's multi_logloss: 0.691844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:29:48,104]\u001b[0m Trial 22 finished with value: 0.7427562041904547 and parameters: {'n_estimators': 137, 'learning_rate': 0.10332682632999363, 'num_leaves': 39, 'max_depth': 1, 'min_child_samples': 17}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.37195\n",
      "[20]\tvalid_0's multi_logloss: 1.25179\n",
      "[30]\tvalid_0's multi_logloss: 1.17186\n",
      "[40]\tvalid_0's multi_logloss: 1.11295\n",
      "[50]\tvalid_0's multi_logloss: 1.06691\n",
      "[60]\tvalid_0's multi_logloss: 1.02973\n",
      "[70]\tvalid_0's multi_logloss: 0.998189\n",
      "[80]\tvalid_0's multi_logloss: 0.97087\n",
      "[90]\tvalid_0's multi_logloss: 0.9472\n",
      "[100]\tvalid_0's multi_logloss: 0.925901\n",
      "[110]\tvalid_0's multi_logloss: 0.907004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:29:54,381]\u001b[0m Trial 23 finished with value: 0.6333602610917589 and parameters: {'n_estimators': 114, 'learning_rate': 0.038283723354248525, 'num_leaves': 47, 'max_depth': 1, 'min_child_samples': 24}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.823795\n",
      "[20]\tvalid_0's multi_logloss: 0.585494\n",
      "[30]\tvalid_0's multi_logloss: 0.470318\n",
      "[40]\tvalid_0's multi_logloss: 0.405222\n",
      "[50]\tvalid_0's multi_logloss: 0.365727\n",
      "[60]\tvalid_0's multi_logloss: 0.339558\n",
      "[70]\tvalid_0's multi_logloss: 0.321923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:30:21,339]\u001b[0m Trial 24 finished with value: 0.8332185705336758 and parameters: {'n_estimators': 71, 'learning_rate': 0.058635069255521685, 'num_leaves': 50, 'max_depth': -1, 'min_child_samples': 11}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.824163\n",
      "[20]\tvalid_0's multi_logloss: 0.672704\n",
      "[30]\tvalid_0's multi_logloss: 0.598149\n",
      "[40]\tvalid_0's multi_logloss: 0.550245\n",
      "[50]\tvalid_0's multi_logloss: 0.51575\n",
      "[60]\tvalid_0's multi_logloss: 0.489492\n",
      "[70]\tvalid_0's multi_logloss: 0.468228\n",
      "[80]\tvalid_0's multi_logloss: 0.450862\n",
      "[90]\tvalid_0's multi_logloss: 0.43643\n",
      "[100]\tvalid_0's multi_logloss: 0.424529\n",
      "[110]\tvalid_0's multi_logloss: 0.413704\n",
      "[120]\tvalid_0's multi_logloss: 0.403529\n",
      "[130]\tvalid_0's multi_logloss: 0.39541\n",
      "[140]\tvalid_0's multi_logloss: 0.388205\n",
      "[150]\tvalid_0's multi_logloss: 0.381126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:30:31,032]\u001b[0m Trial 25 finished with value: 0.8407745676611371 and parameters: {'n_estimators': 150, 'learning_rate': 0.1696620533587489, 'num_leaves': 55, 'max_depth': 3, 'min_child_samples': 33}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.682091\n",
      "[20]\tvalid_0's multi_logloss: 0.498144\n",
      "[30]\tvalid_0's multi_logloss: 0.425023\n",
      "[40]\tvalid_0's multi_logloss: 0.386071\n",
      "[50]\tvalid_0's multi_logloss: 0.362045\n",
      "[60]\tvalid_0's multi_logloss: 0.345358\n",
      "[70]\tvalid_0's multi_logloss: 0.334009\n",
      "[80]\tvalid_0's multi_logloss: 0.32488\n",
      "[90]\tvalid_0's multi_logloss: 0.317808\n",
      "[100]\tvalid_0's multi_logloss: 0.311576\n",
      "[110]\tvalid_0's multi_logloss: 0.306658\n",
      "[120]\tvalid_0's multi_logloss: 0.302228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:31:23,746]\u001b[0m Trial 26 finished with value: 0.8388062753207246 and parameters: {'n_estimators': 126, 'learning_rate': 0.10393772377848207, 'num_leaves': 43, 'max_depth': 20, 'min_child_samples': 5}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.725119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:31:29,757]\u001b[0m Trial 27 finished with value: 0.7047685413529035 and parameters: {'n_estimators': 107, 'learning_rate': 0.26872300564411034, 'num_leaves': 35, 'max_depth': 6, 'min_child_samples': 17}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.675094\n",
      "[20]\tvalid_0's multi_logloss: 0.468914\n",
      "[30]\tvalid_0's multi_logloss: 0.382182\n",
      "[40]\tvalid_0's multi_logloss: 0.336202\n",
      "[50]\tvalid_0's multi_logloss: 0.311272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:31:43,815]\u001b[0m Trial 28 finished with value: 0.841574685833306 and parameters: {'n_estimators': 50, 'learning_rate': 0.08706507450913213, 'num_leaves': 49, 'max_depth': 0, 'min_child_samples': 27}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.690844\n",
      "[20]\tvalid_0's multi_logloss: 0.523062\n",
      "[30]\tvalid_0's multi_logloss: 0.451417\n",
      "[40]\tvalid_0's multi_logloss: 0.410991\n",
      "[50]\tvalid_0's multi_logloss: 0.385976\n",
      "[60]\tvalid_0's multi_logloss: 0.36779\n",
      "[70]\tvalid_0's multi_logloss: 0.354142\n",
      "[80]\tvalid_0's multi_logloss: 0.343711\n",
      "[90]\tvalid_0's multi_logloss: 0.335112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:32:03,217]\u001b[0m Trial 29 finished with value: 0.8426442480033309 and parameters: {'n_estimators': 94, 'learning_rate': 0.12002650072003368, 'num_leaves': 37, 'max_depth': 10, 'min_child_samples': 22}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.799998\n",
      "[20]\tvalid_0's multi_logloss: 0.638782\n",
      "[30]\tvalid_0's multi_logloss: 0.562091\n",
      "[40]\tvalid_0's multi_logloss: 0.515497\n",
      "[50]\tvalid_0's multi_logloss: 0.482175\n",
      "[60]\tvalid_0's multi_logloss: 0.459041\n",
      "[70]\tvalid_0's multi_logloss: 0.440278\n",
      "[80]\tvalid_0's multi_logloss: 0.424619\n",
      "[90]\tvalid_0's multi_logloss: 0.41165\n",
      "[100]\tvalid_0's multi_logloss: 0.401835\n",
      "[110]\tvalid_0's multi_logloss: 0.392686\n",
      "[120]\tvalid_0's multi_logloss: 0.384273\n",
      "[130]\tvalid_0's multi_logloss: 0.377534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:32:22,626]\u001b[0m Trial 30 finished with value: 0.828087397110763 and parameters: {'n_estimators': 138, 'learning_rate': 0.14229753026657363, 'num_leaves': 54, 'max_depth': 5, 'min_child_samples': 15}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.607761\n",
      "[20]\tvalid_0's multi_logloss: 0.461661\n",
      "[30]\tvalid_0's multi_logloss: 0.403142\n",
      "[40]\tvalid_0's multi_logloss: 0.371712\n",
      "[50]\tvalid_0's multi_logloss: 0.351576\n",
      "[60]\tvalid_0's multi_logloss: 0.33557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:32:32,959]\u001b[0m Trial 31 finished with value: 0.8568538114033107 and parameters: {'n_estimators': 63, 'learning_rate': 0.15270388401090046, 'num_leaves': 38, 'max_depth': 10, 'min_child_samples': 49}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.654781\n",
      "[20]\tvalid_0's multi_logloss: 0.502069\n",
      "[30]\tvalid_0's multi_logloss: 0.43772\n",
      "[40]\tvalid_0's multi_logloss: 0.40073\n",
      "[50]\tvalid_0's multi_logloss: 0.376454\n",
      "[60]\tvalid_0's multi_logloss: 0.359074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:32:43,975]\u001b[0m Trial 32 finished with value: 0.837873318604877 and parameters: {'n_estimators': 65, 'learning_rate': 0.14477430524094811, 'num_leaves': 44, 'max_depth': 8, 'min_child_samples': 41}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.878318\n",
      "[20]\tvalid_0's multi_logloss: 0.64535\n",
      "[30]\tvalid_0's multi_logloss: 0.530153\n",
      "[40]\tvalid_0's multi_logloss: 0.463677\n",
      "[50]\tvalid_0's multi_logloss: 0.422806\n",
      "[60]\tvalid_0's multi_logloss: 0.393333\n",
      "[70]\tvalid_0's multi_logloss: 0.373135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:32:57,607]\u001b[0m Trial 33 finished with value: 0.8431853924551984 and parameters: {'n_estimators': 78, 'learning_rate': 0.05756939018977924, 'num_leaves': 38, 'max_depth': 15, 'min_child_samples': 47}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.0821\n",
      "[20]\tvalid_0's multi_logloss: 0.917678\n",
      "[30]\tvalid_0's multi_logloss: 0.826644\n",
      "[40]\tvalid_0's multi_logloss: 0.764209\n",
      "[50]\tvalid_0's multi_logloss: 0.719351\n",
      "[60]\tvalid_0's multi_logloss: 0.684428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:33:03,073]\u001b[0m Trial 34 finished with value: 0.7525392540038961 and parameters: {'n_estimators': 65, 'learning_rate': 0.08586454037841107, 'num_leaves': 42, 'max_depth': 2, 'min_child_samples': 28}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.593192\n",
      "[20]\tvalid_0's multi_logloss: 0.454415\n",
      "[30]\tvalid_0's multi_logloss: 0.398002\n",
      "[40]\tvalid_0's multi_logloss: 0.367366\n",
      "[50]\tvalid_0's multi_logloss: 0.348177\n",
      "[60]\tvalid_0's multi_logloss: 0.33445\n",
      "[70]\tvalid_0's multi_logloss: 0.324068\n",
      "[80]\tvalid_0's multi_logloss: 0.316387\n",
      "[90]\tvalid_0's multi_logloss: 0.309752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:33:17,391]\u001b[0m Trial 35 finished with value: 0.8515304229097007 and parameters: {'n_estimators': 95, 'learning_rate': 0.16246348377831438, 'num_leaves': 34, 'max_depth': 10, 'min_child_samples': 43}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.53345\n",
      "[20]\tvalid_0's multi_logloss: 0.566689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:33:22,635]\u001b[0m Trial 36 finished with value: 0.7804022915362971 and parameters: {'n_estimators': 119, 'learning_rate': 0.25602962558740316, 'num_leaves': 24, 'max_depth': 11, 'min_child_samples': 50}. Best is trial 20 with value: 0.8629610968444204.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.523622\n",
      "[20]\tvalid_0's multi_logloss: 0.403517\n",
      "[30]\tvalid_0's multi_logloss: 0.358771\n",
      "[40]\tvalid_0's multi_logloss: 0.334902\n",
      "[50]\tvalid_0's multi_logloss: 0.319536\n",
      "[60]\tvalid_0's multi_logloss: 0.309135\n",
      "[70]\tvalid_0's multi_logloss: 0.302357\n",
      "[80]\tvalid_0's multi_logloss: 0.297344\n",
      "[90]\tvalid_0's multi_logloss: 0.293475\n",
      "[100]\tvalid_0's multi_logloss: 0.290988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:33:38,667]\u001b[0m Trial 37 finished with value: 0.8642686797311919 and parameters: {'n_estimators': 108, 'learning_rate': 0.19483194555965722, 'num_leaves': 33, 'max_depth': 14, 'min_child_samples': 45}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.520868\n",
      "[20]\tvalid_0's multi_logloss: 0.404093\n",
      "[30]\tvalid_0's multi_logloss: 0.358336\n",
      "[40]\tvalid_0's multi_logloss: 0.335578\n",
      "[50]\tvalid_0's multi_logloss: 0.321423\n",
      "[60]\tvalid_0's multi_logloss: 0.31215\n",
      "[70]\tvalid_0's multi_logloss: 0.30575\n",
      "[80]\tvalid_0's multi_logloss: 0.300754\n",
      "[90]\tvalid_0's multi_logloss: 0.297371\n",
      "[100]\tvalid_0's multi_logloss: 0.294235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:33:54,769]\u001b[0m Trial 38 finished with value: 0.8598975041183949 and parameters: {'n_estimators': 108, 'learning_rate': 0.2012590913887239, 'num_leaves': 32, 'max_depth': 14, 'min_child_samples': 44}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.536058\n",
      "[20]\tvalid_0's multi_logloss: 0.409434\n",
      "[30]\tvalid_0's multi_logloss: 0.360721\n",
      "[40]\tvalid_0's multi_logloss: 0.338628\n",
      "[50]\tvalid_0's multi_logloss: 0.322341\n",
      "[60]\tvalid_0's multi_logloss: 0.3118\n",
      "[70]\tvalid_0's multi_logloss: 0.305767\n",
      "[80]\tvalid_0's multi_logloss: 0.299985\n",
      "[90]\tvalid_0's multi_logloss: 0.295204\n",
      "[100]\tvalid_0's multi_logloss: 0.291864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:34:10,851]\u001b[0m Trial 39 finished with value: 0.8596802163301107 and parameters: {'n_estimators': 108, 'learning_rate': 0.18615599509384173, 'num_leaves': 32, 'max_depth': 14, 'min_child_samples': 44}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.519737\n",
      "[20]\tvalid_0's multi_logloss: 0.404826\n",
      "[30]\tvalid_0's multi_logloss: 0.359761\n",
      "[40]\tvalid_0's multi_logloss: 0.335357\n",
      "[50]\tvalid_0's multi_logloss: 0.320861\n",
      "[60]\tvalid_0's multi_logloss: 0.311023\n",
      "[70]\tvalid_0's multi_logloss: 0.303919\n",
      "[80]\tvalid_0's multi_logloss: 0.300125\n",
      "[90]\tvalid_0's multi_logloss: 0.296301\n",
      "[100]\tvalid_0's multi_logloss: 0.293811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:34:27,090]\u001b[0m Trial 40 finished with value: 0.8515209963716699 and parameters: {'n_estimators': 107, 'learning_rate': 0.21792394746337437, 'num_leaves': 31, 'max_depth': 14, 'min_child_samples': 39}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.58958\n",
      "[20]\tvalid_0's multi_logloss: 0.749122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:34:31,894]\u001b[0m Trial 41 finished with value: 0.7575243198888206 and parameters: {'n_estimators': 109, 'learning_rate': 0.32158547499327267, 'num_leaves': 25, 'max_depth': 14, 'min_child_samples': 43}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.525179\n",
      "[20]\tvalid_0's multi_logloss: 0.396377\n",
      "[30]\tvalid_0's multi_logloss: 0.349619\n",
      "[40]\tvalid_0's multi_logloss: 0.325016\n",
      "[50]\tvalid_0's multi_logloss: 0.31014\n",
      "[60]\tvalid_0's multi_logloss: 0.299305\n",
      "[70]\tvalid_0's multi_logloss: 0.29261\n",
      "[80]\tvalid_0's multi_logloss: 0.287766\n",
      "[90]\tvalid_0's multi_logloss: 0.283931\n",
      "[100]\tvalid_0's multi_logloss: 0.282066\n",
      "[110]\tvalid_0's multi_logloss: 0.280171\n",
      "[120]\tvalid_0's multi_logloss: 0.279093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:34:49,202]\u001b[0m Trial 42 finished with value: 0.863346207480407 and parameters: {'n_estimators': 123, 'learning_rate': 0.18321044652513307, 'num_leaves': 32, 'max_depth': 17, 'min_child_samples': 50}. Best is trial 37 with value: 0.8642686797311919.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.562301\n",
      "[20]\tvalid_0's multi_logloss: 0.423294\n",
      "[30]\tvalid_0's multi_logloss: 0.368756\n",
      "[40]\tvalid_0's multi_logloss: 0.340303\n",
      "[50]\tvalid_0's multi_logloss: 0.322047\n",
      "[60]\tvalid_0's multi_logloss: 0.310186\n",
      "[70]\tvalid_0's multi_logloss: 0.302181\n",
      "[80]\tvalid_0's multi_logloss: 0.296421\n",
      "[90]\tvalid_0's multi_logloss: 0.293025\n",
      "[100]\tvalid_0's multi_logloss: 0.289818\n",
      "[110]\tvalid_0's multi_logloss: 0.285797\n",
      "[120]\tvalid_0's multi_logloss: 0.283931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:04,316]\u001b[0m Trial 43 finished with value: 0.8744125645367014 and parameters: {'n_estimators': 121, 'learning_rate': 0.18636561602904742, 'num_leaves': 20, 'max_depth': 17, 'min_child_samples': 45}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.55564\n",
      "[20]\tvalid_0's multi_logloss: 0.423121\n",
      "[30]\tvalid_0's multi_logloss: 0.370253\n",
      "[40]\tvalid_0's multi_logloss: 0.343185\n",
      "[50]\tvalid_0's multi_logloss: 0.326575\n",
      "[60]\tvalid_0's multi_logloss: 0.314131\n",
      "[70]\tvalid_0's multi_logloss: 0.30636\n",
      "[80]\tvalid_0's multi_logloss: 0.300152\n",
      "[90]\tvalid_0's multi_logloss: 0.29647\n",
      "[100]\tvalid_0's multi_logloss: 0.293382\n",
      "[110]\tvalid_0's multi_logloss: 0.290833\n",
      "[120]\tvalid_0's multi_logloss: 0.289592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:18,403]\u001b[0m Trial 44 finished with value: 0.865760800203885 and parameters: {'n_estimators': 121, 'learning_rate': 0.2020282804882548, 'num_leaves': 18, 'max_depth': 17, 'min_child_samples': 46}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.583415\n",
      "[20]\tvalid_0's multi_logloss: 0.662623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:22,642]\u001b[0m Trial 45 finished with value: 0.7352549637412629 and parameters: {'n_estimators': 120, 'learning_rate': 0.3542384809071993, 'num_leaves': 16, 'max_depth': 17, 'min_child_samples': 47}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.559443\n",
      "[20]\tvalid_0's multi_logloss: 0.556627\n",
      "[30]\tvalid_0's multi_logloss: 0.501394\n",
      "[40]\tvalid_0's multi_logloss: 0.59777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:29,486]\u001b[0m Trial 46 finished with value: 0.7782650061306756 and parameters: {'n_estimators': 132, 'learning_rate': 0.23751543796911206, 'num_leaves': 20, 'max_depth': 18, 'min_child_samples': 39}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.671566\n",
      "[20]\tvalid_0's multi_logloss: 0.497121\n",
      "[30]\tvalid_0's multi_logloss: 0.425589\n",
      "[40]\tvalid_0's multi_logloss: 0.384228\n",
      "[50]\tvalid_0's multi_logloss: 0.358287\n",
      "[60]\tvalid_0's multi_logloss: 0.340338\n",
      "[70]\tvalid_0's multi_logloss: 0.327589\n",
      "[80]\tvalid_0's multi_logloss: 0.316973\n",
      "[90]\tvalid_0's multi_logloss: 0.309592\n",
      "[100]\tvalid_0's multi_logloss: 0.303034\n",
      "[110]\tvalid_0's multi_logloss: 0.29804\n",
      "[120]\tvalid_0's multi_logloss: 0.293931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:43,190]\u001b[0m Trial 47 finished with value: 0.8704220560799043 and parameters: {'n_estimators': 123, 'learning_rate': 0.12887870505203589, 'num_leaves': 17, 'max_depth': 20, 'min_child_samples': 50}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.601408\n",
      "[20]\tvalid_0's multi_logloss: 0.455128\n",
      "[30]\tvalid_0's multi_logloss: 0.393532\n",
      "[40]\tvalid_0's multi_logloss: 0.360436\n",
      "[50]\tvalid_0's multi_logloss: 0.339269\n",
      "[60]\tvalid_0's multi_logloss: 0.324492\n",
      "[70]\tvalid_0's multi_logloss: 0.314059\n",
      "[80]\tvalid_0's multi_logloss: 0.30615\n",
      "[90]\tvalid_0's multi_logloss: 0.299635\n",
      "[100]\tvalid_0's multi_logloss: 0.295731\n",
      "[110]\tvalid_0's multi_logloss: 0.291716\n",
      "[120]\tvalid_0's multi_logloss: 0.288795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:35:56,918]\u001b[0m Trial 48 finished with value: 0.8658939642462495 and parameters: {'n_estimators': 124, 'learning_rate': 0.17514502040138497, 'num_leaves': 16, 'max_depth': 20, 'min_child_samples': 46}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.690225\n",
      "[20]\tvalid_0's multi_logloss: 0.508869\n",
      "[30]\tvalid_0's multi_logloss: 0.43267\n",
      "[40]\tvalid_0's multi_logloss: 0.390562\n",
      "[50]\tvalid_0's multi_logloss: 0.363283\n",
      "[60]\tvalid_0's multi_logloss: 0.34479\n",
      "[70]\tvalid_0's multi_logloss: 0.331408\n",
      "[80]\tvalid_0's multi_logloss: 0.321496\n",
      "[90]\tvalid_0's multi_logloss: 0.312544\n",
      "[100]\tvalid_0's multi_logloss: 0.306269\n",
      "[110]\tvalid_0's multi_logloss: 0.301063\n",
      "[120]\tvalid_0's multi_logloss: 0.296215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:36:11,674]\u001b[0m Trial 49 finished with value: 0.8687338733792964 and parameters: {'n_estimators': 128, 'learning_rate': 0.12032061184140726, 'num_leaves': 17, 'max_depth': 20, 'min_child_samples': 46}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.679431\n",
      "[20]\tvalid_0's multi_logloss: 0.505916\n",
      "[30]\tvalid_0's multi_logloss: 0.431612\n",
      "[40]\tvalid_0's multi_logloss: 0.390566\n",
      "[50]\tvalid_0's multi_logloss: 0.363616\n",
      "[60]\tvalid_0's multi_logloss: 0.346689\n",
      "[70]\tvalid_0's multi_logloss: 0.333522\n",
      "[80]\tvalid_0's multi_logloss: 0.324104\n",
      "[90]\tvalid_0's multi_logloss: 0.315615\n",
      "[100]\tvalid_0's multi_logloss: 0.309496\n",
      "[110]\tvalid_0's multi_logloss: 0.304496\n",
      "[120]\tvalid_0's multi_logloss: 0.299718\n",
      "[130]\tvalid_0's multi_logloss: 0.296205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:36:25,991]\u001b[0m Trial 50 finished with value: 0.8672284681589082 and parameters: {'n_estimators': 130, 'learning_rate': 0.12915351041400205, 'num_leaves': 16, 'max_depth': 20, 'min_child_samples': 47}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.683088\n",
      "[20]\tvalid_0's multi_logloss: 0.50809\n",
      "[30]\tvalid_0's multi_logloss: 0.433012\n",
      "[40]\tvalid_0's multi_logloss: 0.390165\n",
      "[50]\tvalid_0's multi_logloss: 0.364022\n",
      "[60]\tvalid_0's multi_logloss: 0.345297\n",
      "[70]\tvalid_0's multi_logloss: 0.331799\n",
      "[80]\tvalid_0's multi_logloss: 0.320977\n",
      "[90]\tvalid_0's multi_logloss: 0.312963\n",
      "[100]\tvalid_0's multi_logloss: 0.306598\n",
      "[110]\tvalid_0's multi_logloss: 0.301317\n",
      "[120]\tvalid_0's multi_logloss: 0.296855\n",
      "[130]\tvalid_0's multi_logloss: 0.293189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:36:40,405]\u001b[0m Trial 51 finished with value: 0.8641109037076302 and parameters: {'n_estimators': 130, 'learning_rate': 0.12728928062865585, 'num_leaves': 16, 'max_depth': 20, 'min_child_samples': 46}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.754079\n",
      "[20]\tvalid_0's multi_logloss: 0.576366\n",
      "[30]\tvalid_0's multi_logloss: 0.496591\n",
      "[40]\tvalid_0's multi_logloss: 0.449666\n",
      "[50]\tvalid_0's multi_logloss: 0.41762\n",
      "[60]\tvalid_0's multi_logloss: 0.394606\n",
      "[70]\tvalid_0's multi_logloss: 0.376732\n",
      "[80]\tvalid_0's multi_logloss: 0.362765\n",
      "[90]\tvalid_0's multi_logloss: 0.351732\n",
      "[100]\tvalid_0's multi_logloss: 0.342696\n",
      "[110]\tvalid_0's multi_logloss: 0.335888\n",
      "[120]\tvalid_0's multi_logloss: 0.329372\n",
      "[130]\tvalid_0's multi_logloss: 0.32367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:36:52,591]\u001b[0m Trial 52 finished with value: 0.8601820703726659 and parameters: {'n_estimators': 135, 'learning_rate': 0.12354304164874312, 'num_leaves': 11, 'max_depth': 19, 'min_child_samples': 48}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.626354\n",
      "[20]\tvalid_0's multi_logloss: 0.467362\n",
      "[30]\tvalid_0's multi_logloss: 0.40167\n",
      "[40]\tvalid_0's multi_logloss: 0.365197\n",
      "[50]\tvalid_0's multi_logloss: 0.344204\n",
      "[60]\tvalid_0's multi_logloss: 0.328419\n",
      "[70]\tvalid_0's multi_logloss: 0.316504\n",
      "[80]\tvalid_0's multi_logloss: 0.307658\n",
      "[90]\tvalid_0's multi_logloss: 0.301796\n",
      "[100]\tvalid_0's multi_logloss: 0.296577\n",
      "[110]\tvalid_0's multi_logloss: 0.291781\n",
      "[120]\tvalid_0's multi_logloss: 0.289798\n",
      "[130]\tvalid_0's multi_logloss: 0.286795\n",
      "[140]\tvalid_0's multi_logloss: 0.284832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:37:09,131]\u001b[0m Trial 53 finished with value: 0.8719249287301751 and parameters: {'n_estimators': 141, 'learning_rate': 0.15895665956209817, 'num_leaves': 17, 'max_depth': 19, 'min_child_samples': 40}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.652039\n",
      "[20]\tvalid_0's multi_logloss: 0.49688\n",
      "[30]\tvalid_0's multi_logloss: 0.427765\n",
      "[40]\tvalid_0's multi_logloss: 0.38913\n",
      "[50]\tvalid_0's multi_logloss: 0.363863\n",
      "[60]\tvalid_0's multi_logloss: 0.349065\n",
      "[70]\tvalid_0's multi_logloss: 0.336051\n",
      "[80]\tvalid_0's multi_logloss: 0.325461\n",
      "[90]\tvalid_0's multi_logloss: 0.317823\n",
      "[100]\tvalid_0's multi_logloss: 0.311675\n",
      "[110]\tvalid_0's multi_logloss: 0.306718\n",
      "[120]\tvalid_0's multi_logloss: 0.3022\n",
      "[130]\tvalid_0's multi_logloss: 0.298358\n",
      "[140]\tvalid_0's multi_logloss: 0.296024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:37:24,510]\u001b[0m Trial 54 finished with value: 0.8706262827412987 and parameters: {'n_estimators': 142, 'learning_rate': 0.1588215753163202, 'num_leaves': 14, 'max_depth': 19, 'min_child_samples': 39}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.695591\n",
      "[20]\tvalid_0's multi_logloss: 0.527675\n",
      "[30]\tvalid_0's multi_logloss: 0.452726\n",
      "[40]\tvalid_0's multi_logloss: 0.409947\n",
      "[50]\tvalid_0's multi_logloss: 0.381601\n",
      "[60]\tvalid_0's multi_logloss: 0.361541\n",
      "[70]\tvalid_0's multi_logloss: 0.347299\n",
      "[80]\tvalid_0's multi_logloss: 0.335947\n",
      "[90]\tvalid_0's multi_logloss: 0.32684\n",
      "[100]\tvalid_0's multi_logloss: 0.320112\n",
      "[110]\tvalid_0's multi_logloss: 0.313639\n",
      "[120]\tvalid_0's multi_logloss: 0.309009\n",
      "[130]\tvalid_0's multi_logloss: 0.304208\n",
      "[140]\tvalid_0's multi_logloss: 0.30033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:37:39,202]\u001b[0m Trial 55 finished with value: 0.8719986334679257 and parameters: {'n_estimators': 144, 'learning_rate': 0.1393382692218996, 'num_leaves': 13, 'max_depth': 19, 'min_child_samples': 41}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.653812\n",
      "[20]\tvalid_0's multi_logloss: 0.497329\n",
      "[30]\tvalid_0's multi_logloss: 0.427904\n",
      "[40]\tvalid_0's multi_logloss: 0.389299\n",
      "[50]\tvalid_0's multi_logloss: 0.364858\n",
      "[60]\tvalid_0's multi_logloss: 0.346259\n",
      "[70]\tvalid_0's multi_logloss: 0.333948\n",
      "[80]\tvalid_0's multi_logloss: 0.323276\n",
      "[90]\tvalid_0's multi_logloss: 0.315991\n",
      "[100]\tvalid_0's multi_logloss: 0.309606\n",
      "[110]\tvalid_0's multi_logloss: 0.305061\n",
      "[120]\tvalid_0's multi_logloss: 0.301214\n",
      "[130]\tvalid_0's multi_logloss: 0.29772\n",
      "[140]\tvalid_0's multi_logloss: 0.294805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:37:54,587]\u001b[0m Trial 56 finished with value: 0.8690372895885058 and parameters: {'n_estimators': 143, 'learning_rate': 0.1566149749225125, 'num_leaves': 14, 'max_depth': 18, 'min_child_samples': 40}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.67314\n",
      "[20]\tvalid_0's multi_logloss: 0.511314\n",
      "[30]\tvalid_0's multi_logloss: 0.440654\n",
      "[40]\tvalid_0's multi_logloss: 0.400448\n",
      "[50]\tvalid_0's multi_logloss: 0.374614\n",
      "[60]\tvalid_0's multi_logloss: 0.355383\n",
      "[70]\tvalid_0's multi_logloss: 0.34272\n",
      "[80]\tvalid_0's multi_logloss: 0.331772\n",
      "[90]\tvalid_0's multi_logloss: 0.323362\n",
      "[100]\tvalid_0's multi_logloss: 0.3174\n",
      "[110]\tvalid_0's multi_logloss: 0.312509\n",
      "[120]\tvalid_0's multi_logloss: 0.308216\n",
      "[130]\tvalid_0's multi_logloss: 0.303281\n",
      "[140]\tvalid_0's multi_logloss: 0.299805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:38:11,714]\u001b[0m Trial 57 finished with value: 0.8684720314570125 and parameters: {'n_estimators': 141, 'learning_rate': 0.15281019545499006, 'num_leaves': 13, 'max_depth': 18, 'min_child_samples': 40}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.681211\n",
      "[20]\tvalid_0's multi_logloss: 0.63023\n",
      "[30]\tvalid_0's multi_logloss: 0.711084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:38:18,045]\u001b[0m Trial 58 finished with value: 0.736309891123152 and parameters: {'n_estimators': 144, 'learning_rate': 0.2717316632439302, 'num_leaves': 13, 'max_depth': 19, 'min_child_samples': 37}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.529814\n",
      "[20]\tvalid_0's multi_logloss: 0.413741\n",
      "[30]\tvalid_0's multi_logloss: 0.366463\n",
      "[40]\tvalid_0's multi_logloss: 0.340413\n",
      "[50]\tvalid_0's multi_logloss: 0.325314\n",
      "[60]\tvalid_0's multi_logloss: 0.315254\n",
      "[70]\tvalid_0's multi_logloss: 0.308991\n",
      "[80]\tvalid_0's multi_logloss: 0.303518\n",
      "[90]\tvalid_0's multi_logloss: 0.300208\n",
      "[100]\tvalid_0's multi_logloss: 0.297215\n",
      "[110]\tvalid_0's multi_logloss: 0.294564\n",
      "[120]\tvalid_0's multi_logloss: 0.292716\n",
      "[130]\tvalid_0's multi_logloss: 0.291275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:38:40,096]\u001b[0m Trial 59 finished with value: 0.8611939409257849 and parameters: {'n_estimators': 139, 'learning_rate': 0.22648493024477784, 'num_leaves': 22, 'max_depth': 16, 'min_child_samples': 34}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.819692\n",
      "[20]\tvalid_0's multi_logloss: 0.620889\n",
      "[30]\tvalid_0's multi_logloss: 0.526657\n",
      "[40]\tvalid_0's multi_logloss: 0.470523\n",
      "[50]\tvalid_0's multi_logloss: 0.434651\n",
      "[60]\tvalid_0's multi_logloss: 0.407651\n",
      "[70]\tvalid_0's multi_logloss: 0.387411\n",
      "[80]\tvalid_0's multi_logloss: 0.371501\n",
      "[90]\tvalid_0's multi_logloss: 0.359328\n",
      "[100]\tvalid_0's multi_logloss: 0.348473\n",
      "[110]\tvalid_0's multi_logloss: 0.340093\n",
      "[120]\tvalid_0's multi_logloss: 0.333074\n",
      "[130]\tvalid_0's multi_logloss: 0.326599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:38:55,982]\u001b[0m Trial 60 finished with value: 0.8645762796918397 and parameters: {'n_estimators': 134, 'learning_rate': 0.09032300582993841, 'num_leaves': 13, 'max_depth': 18, 'min_child_samples': 38}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.688056\n",
      "[20]\tvalid_0's multi_logloss: 0.506836\n",
      "[30]\tvalid_0's multi_logloss: 0.430375\n",
      "[40]\tvalid_0's multi_logloss: 0.386746\n",
      "[50]\tvalid_0's multi_logloss: 0.359513\n",
      "[60]\tvalid_0's multi_logloss: 0.341277\n",
      "[70]\tvalid_0's multi_logloss: 0.328358\n",
      "[80]\tvalid_0's multi_logloss: 0.317301\n",
      "[90]\tvalid_0's multi_logloss: 0.309779\n",
      "[100]\tvalid_0's multi_logloss: 0.303867\n",
      "[110]\tvalid_0's multi_logloss: 0.298768\n",
      "[120]\tvalid_0's multi_logloss: 0.294345\n",
      "[130]\tvalid_0's multi_logloss: 0.291845\n",
      "[140]\tvalid_0's multi_logloss: 0.288729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:39:13,139]\u001b[0m Trial 61 finished with value: 0.8740523567860273 and parameters: {'n_estimators': 142, 'learning_rate': 0.11887454844508036, 'num_leaves': 18, 'max_depth': 19, 'min_child_samples': 41}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.80279\n",
      "[20]\tvalid_0's multi_logloss: 0.617524\n",
      "[30]\tvalid_0's multi_logloss: 0.530619\n",
      "[40]\tvalid_0's multi_logloss: 0.479676\n",
      "[50]\tvalid_0's multi_logloss: 0.445118\n",
      "[60]\tvalid_0's multi_logloss: 0.419366\n",
      "[70]\tvalid_0's multi_logloss: 0.400066\n",
      "[80]\tvalid_0's multi_logloss: 0.384609\n",
      "[90]\tvalid_0's multi_logloss: 0.371094\n",
      "[100]\tvalid_0's multi_logloss: 0.360603\n",
      "[110]\tvalid_0's multi_logloss: 0.352193\n",
      "[120]\tvalid_0's multi_logloss: 0.344752\n",
      "[130]\tvalid_0's multi_logloss: 0.337989\n",
      "[140]\tvalid_0's multi_logloss: 0.332051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:39:26,091]\u001b[0m Trial 62 finished with value: 0.8586143347709585 and parameters: {'n_estimators': 143, 'learning_rate': 0.11066545225201033, 'num_leaves': 10, 'max_depth': 19, 'min_child_samples': 41}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.617527\n",
      "[20]\tvalid_0's multi_logloss: 0.459146\n",
      "[30]\tvalid_0's multi_logloss: 0.39524\n",
      "[40]\tvalid_0's multi_logloss: 0.359901\n",
      "[50]\tvalid_0's multi_logloss: 0.335759\n",
      "[60]\tvalid_0's multi_logloss: 0.32017\n",
      "[70]\tvalid_0's multi_logloss: 0.310031\n",
      "[80]\tvalid_0's multi_logloss: 0.302089\n",
      "[90]\tvalid_0's multi_logloss: 0.295488\n",
      "[100]\tvalid_0's multi_logloss: 0.290615\n",
      "[110]\tvalid_0's multi_logloss: 0.286736\n",
      "[120]\tvalid_0's multi_logloss: 0.283856\n",
      "[130]\tvalid_0's multi_logloss: 0.281811\n",
      "[140]\tvalid_0's multi_logloss: 0.279713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:39:47,248]\u001b[0m Trial 63 finished with value: 0.8707482915160545 and parameters: {'n_estimators': 147, 'learning_rate': 0.14324129802726113, 'num_leaves': 22, 'max_depth': 16, 'min_child_samples': 31}. Best is trial 43 with value: 0.8744125645367014.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.735964\n",
      "[20]\tvalid_0's multi_logloss: 0.537559\n",
      "[30]\tvalid_0's multi_logloss: 0.450856\n",
      "[40]\tvalid_0's multi_logloss: 0.402134\n",
      "[50]\tvalid_0's multi_logloss: 0.371814\n",
      "[60]\tvalid_0's multi_logloss: 0.351216\n",
      "[70]\tvalid_0's multi_logloss: 0.335882\n",
      "[80]\tvalid_0's multi_logloss: 0.324779\n",
      "[90]\tvalid_0's multi_logloss: 0.315331\n",
      "[100]\tvalid_0's multi_logloss: 0.308527\n",
      "[110]\tvalid_0's multi_logloss: 0.302539\n",
      "[120]\tvalid_0's multi_logloss: 0.296975\n",
      "[130]\tvalid_0's multi_logloss: 0.292863\n",
      "[140]\tvalid_0's multi_logloss: 0.28901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:40:08,509]\u001b[0m Trial 64 finished with value: 0.8751316406492413 and parameters: {'n_estimators': 147, 'learning_rate': 0.09549382412586092, 'num_leaves': 22, 'max_depth': 16, 'min_child_samples': 33}. Best is trial 64 with value: 0.8751316406492413.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.81131\n",
      "[20]\tvalid_0's multi_logloss: 0.596274\n",
      "[30]\tvalid_0's multi_logloss: 0.495431\n",
      "[40]\tvalid_0's multi_logloss: 0.438177\n",
      "[50]\tvalid_0's multi_logloss: 0.4015\n",
      "[60]\tvalid_0's multi_logloss: 0.375486\n",
      "[70]\tvalid_0's multi_logloss: 0.357279\n",
      "[80]\tvalid_0's multi_logloss: 0.342729\n",
      "[90]\tvalid_0's multi_logloss: 0.331766\n",
      "[100]\tvalid_0's multi_logloss: 0.322524\n",
      "[110]\tvalid_0's multi_logloss: 0.314384\n",
      "[120]\tvalid_0's multi_logloss: 0.307849\n",
      "[130]\tvalid_0's multi_logloss: 0.303587\n",
      "[140]\tvalid_0's multi_logloss: 0.299138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:40:33,488]\u001b[0m Trial 65 finished with value: 0.8709782749840119 and parameters: {'n_estimators': 148, 'learning_rate': 0.07572182639650163, 'num_leaves': 22, 'max_depth': 16, 'min_child_samples': 32}. Best is trial 64 with value: 0.8751316406492413.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.787034\n",
      "[20]\tvalid_0's multi_logloss: 0.572442\n",
      "[30]\tvalid_0's multi_logloss: 0.474827\n",
      "[40]\tvalid_0's multi_logloss: 0.420372\n",
      "[50]\tvalid_0's multi_logloss: 0.386005\n",
      "[60]\tvalid_0's multi_logloss: 0.363177\n",
      "[70]\tvalid_0's multi_logloss: 0.345393\n",
      "[80]\tvalid_0's multi_logloss: 0.332564\n",
      "[90]\tvalid_0's multi_logloss: 0.322327\n",
      "[100]\tvalid_0's multi_logloss: 0.313337\n",
      "[110]\tvalid_0's multi_logloss: 0.307158\n",
      "[120]\tvalid_0's multi_logloss: 0.30158\n",
      "[130]\tvalid_0's multi_logloss: 0.297011\n",
      "[140]\tvalid_0's multi_logloss: 0.293234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:41:02,141]\u001b[0m Trial 66 finished with value: 0.8674476655239096 and parameters: {'n_estimators': 148, 'learning_rate': 0.07733454196348283, 'num_leaves': 28, 'max_depth': 16, 'min_child_samples': 30}. Best is trial 64 with value: 0.8751316406492413.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.72534\n",
      "[20]\tvalid_0's multi_logloss: 0.529104\n",
      "[30]\tvalid_0's multi_logloss: 0.443425\n",
      "[40]\tvalid_0's multi_logloss: 0.396543\n",
      "[50]\tvalid_0's multi_logloss: 0.366653\n",
      "[60]\tvalid_0's multi_logloss: 0.345133\n",
      "[70]\tvalid_0's multi_logloss: 0.330396\n",
      "[80]\tvalid_0's multi_logloss: 0.319356\n",
      "[90]\tvalid_0's multi_logloss: 0.310372\n",
      "[100]\tvalid_0's multi_logloss: 0.303301\n",
      "[110]\tvalid_0's multi_logloss: 0.297346\n",
      "[120]\tvalid_0's multi_logloss: 0.292231\n",
      "[130]\tvalid_0's multi_logloss: 0.288351\n",
      "[140]\tvalid_0's multi_logloss: 0.285116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:41:25,682]\u001b[0m Trial 67 finished with value: 0.879435525089546 and parameters: {'n_estimators': 147, 'learning_rate': 0.0980999765200384, 'num_leaves': 23, 'max_depth': 16, 'min_child_samples': 32}. Best is trial 67 with value: 0.879435525089546.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.769831\n",
      "[20]\tvalid_0's multi_logloss: 0.566061\n",
      "[30]\tvalid_0's multi_logloss: 0.47448\n",
      "[40]\tvalid_0's multi_logloss: 0.422382\n",
      "[50]\tvalid_0's multi_logloss: 0.389465\n",
      "[60]\tvalid_0's multi_logloss: 0.366672\n",
      "[70]\tvalid_0's multi_logloss: 0.349517\n",
      "[80]\tvalid_0's multi_logloss: 0.336733\n",
      "[90]\tvalid_0's multi_logloss: 0.326109\n",
      "[100]\tvalid_0's multi_logloss: 0.317983\n",
      "[110]\tvalid_0's multi_logloss: 0.311092\n",
      "[120]\tvalid_0's multi_logloss: 0.305051\n",
      "[130]\tvalid_0's multi_logloss: 0.30017\n",
      "[140]\tvalid_0's multi_logloss: 0.296372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:41:45,968]\u001b[0m Trial 68 finished with value: 0.872805947860421 and parameters: {'n_estimators': 146, 'learning_rate': 0.08886551455932636, 'num_leaves': 20, 'max_depth': 15, 'min_child_samples': 33}. Best is trial 67 with value: 0.879435525089546.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.758086\n",
      "[20]\tvalid_0's multi_logloss: 0.56114\n",
      "[30]\tvalid_0's multi_logloss: 0.474239\n",
      "[40]\tvalid_0's multi_logloss: 0.42446\n",
      "[50]\tvalid_0's multi_logloss: 0.392282\n",
      "[60]\tvalid_0's multi_logloss: 0.368301\n",
      "[70]\tvalid_0's multi_logloss: 0.352057\n",
      "[80]\tvalid_0's multi_logloss: 0.339694\n",
      "[90]\tvalid_0's multi_logloss: 0.329927\n",
      "[100]\tvalid_0's multi_logloss: 0.321803\n",
      "[110]\tvalid_0's multi_logloss: 0.314171\n",
      "[120]\tvalid_0's multi_logloss: 0.308195\n",
      "[130]\tvalid_0's multi_logloss: 0.303052\n",
      "[140]\tvalid_0's multi_logloss: 0.298475\n",
      "[150]\tvalid_0's multi_logloss: 0.294643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:42:05,950]\u001b[0m Trial 69 finished with value: 0.8729846110462693 and parameters: {'n_estimators': 150, 'learning_rate': 0.09478130148113297, 'num_leaves': 20, 'max_depth': 13, 'min_child_samples': 35}. Best is trial 67 with value: 0.879435525089546.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.758983\n",
      "[20]\tvalid_0's multi_logloss: 0.561973\n",
      "[30]\tvalid_0's multi_logloss: 0.473693\n",
      "[40]\tvalid_0's multi_logloss: 0.423801\n",
      "[50]\tvalid_0's multi_logloss: 0.391196\n",
      "[60]\tvalid_0's multi_logloss: 0.368615\n",
      "[70]\tvalid_0's multi_logloss: 0.351999\n",
      "[80]\tvalid_0's multi_logloss: 0.338983\n",
      "[90]\tvalid_0's multi_logloss: 0.328583\n",
      "[100]\tvalid_0's multi_logloss: 0.320646\n",
      "[110]\tvalid_0's multi_logloss: 0.313131\n",
      "[120]\tvalid_0's multi_logloss: 0.307735\n",
      "[130]\tvalid_0's multi_logloss: 0.301959\n",
      "[140]\tvalid_0's multi_logloss: 0.297532\n",
      "[150]\tvalid_0's multi_logloss: 0.294114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:42:26,016]\u001b[0m Trial 70 finished with value: 0.8747903846887553 and parameters: {'n_estimators': 150, 'learning_rate': 0.09442905708767964, 'num_leaves': 20, 'max_depth': 13, 'min_child_samples': 35}. Best is trial 67 with value: 0.879435525089546.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.726279\n",
      "[20]\tvalid_0's multi_logloss: 0.540229\n",
      "[30]\tvalid_0's multi_logloss: 0.4573\n",
      "[40]\tvalid_0's multi_logloss: 0.411331\n",
      "[50]\tvalid_0's multi_logloss: 0.381611\n",
      "[60]\tvalid_0's multi_logloss: 0.360758\n",
      "[70]\tvalid_0's multi_logloss: 0.345459\n",
      "[80]\tvalid_0's multi_logloss: 0.333706\n",
      "[90]\tvalid_0's multi_logloss: 0.323556\n",
      "[100]\tvalid_0's multi_logloss: 0.31559\n",
      "[110]\tvalid_0's multi_logloss: 0.309176\n",
      "[120]\tvalid_0's multi_logloss: 0.303208\n",
      "[130]\tvalid_0's multi_logloss: 0.298744\n",
      "[140]\tvalid_0's multi_logloss: 0.29493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:42:45,268]\u001b[0m Trial 71 finished with value: 0.8810595533290522 and parameters: {'n_estimators': 149, 'learning_rate': 0.10596416395908402, 'num_leaves': 19, 'max_depth': 13, 'min_child_samples': 35}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.75932\n",
      "[20]\tvalid_0's multi_logloss: 0.56266\n",
      "[30]\tvalid_0's multi_logloss: 0.474781\n",
      "[40]\tvalid_0's multi_logloss: 0.424036\n",
      "[50]\tvalid_0's multi_logloss: 0.391839\n",
      "[60]\tvalid_0's multi_logloss: 0.368842\n",
      "[70]\tvalid_0's multi_logloss: 0.351998\n",
      "[80]\tvalid_0's multi_logloss: 0.339946\n",
      "[90]\tvalid_0's multi_logloss: 0.329753\n",
      "[100]\tvalid_0's multi_logloss: 0.321021\n",
      "[110]\tvalid_0's multi_logloss: 0.314388\n",
      "[120]\tvalid_0's multi_logloss: 0.308757\n",
      "[130]\tvalid_0's multi_logloss: 0.303673\n",
      "[140]\tvalid_0's multi_logloss: 0.29946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:43:05,183]\u001b[0m Trial 72 finished with value: 0.8703927005710317 and parameters: {'n_estimators': 149, 'learning_rate': 0.09395982321153556, 'num_leaves': 20, 'max_depth': 13, 'min_child_samples': 35}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.732056\n",
      "[20]\tvalid_0's multi_logloss: 0.541182\n",
      "[30]\tvalid_0's multi_logloss: 0.459132\n",
      "[40]\tvalid_0's multi_logloss: 0.413462\n",
      "[50]\tvalid_0's multi_logloss: 0.383719\n",
      "[60]\tvalid_0's multi_logloss: 0.362362\n",
      "[70]\tvalid_0's multi_logloss: 0.346907\n",
      "[80]\tvalid_0's multi_logloss: 0.335267\n",
      "[90]\tvalid_0's multi_logloss: 0.326092\n",
      "[100]\tvalid_0's multi_logloss: 0.31797\n",
      "[110]\tvalid_0's multi_logloss: 0.311769\n",
      "[120]\tvalid_0's multi_logloss: 0.305732\n",
      "[130]\tvalid_0's multi_logloss: 0.301204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:43:24,619]\u001b[0m Trial 73 finished with value: 0.8669344666212155 and parameters: {'n_estimators': 138, 'learning_rate': 0.09930909099100645, 'num_leaves': 24, 'max_depth': 12, 'min_child_samples': 36}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.689843\n",
      "[20]\tvalid_0's multi_logloss: 0.507734\n",
      "[30]\tvalid_0's multi_logloss: 0.431348\n",
      "[40]\tvalid_0's multi_logloss: 0.389007\n",
      "[50]\tvalid_0's multi_logloss: 0.363155\n",
      "[60]\tvalid_0's multi_logloss: 0.344077\n",
      "[70]\tvalid_0's multi_logloss: 0.330085\n",
      "[80]\tvalid_0's multi_logloss: 0.319547\n",
      "[90]\tvalid_0's multi_logloss: 0.311532\n",
      "[100]\tvalid_0's multi_logloss: 0.304359\n",
      "[110]\tvalid_0's multi_logloss: 0.299079\n",
      "[120]\tvalid_0's multi_logloss: 0.294729\n",
      "[130]\tvalid_0's multi_logloss: 0.29057\n",
      "[140]\tvalid_0's multi_logloss: 0.28708\n",
      "[150]\tvalid_0's multi_logloss: 0.284158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:43:47,511]\u001b[0m Trial 74 finished with value: 0.8672262041783182 and parameters: {'n_estimators': 150, 'learning_rate': 0.10891900555802618, 'num_leaves': 27, 'max_depth': 13, 'min_child_samples': 33}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.792183\n",
      "[20]\tvalid_0's multi_logloss: 0.582674\n",
      "[30]\tvalid_0's multi_logloss: 0.488711\n",
      "[40]\tvalid_0's multi_logloss: 0.434112\n",
      "[50]\tvalid_0's multi_logloss: 0.398567\n",
      "[60]\tvalid_0's multi_logloss: 0.373765\n",
      "[70]\tvalid_0's multi_logloss: 0.355357\n",
      "[80]\tvalid_0's multi_logloss: 0.342032\n",
      "[90]\tvalid_0's multi_logloss: 0.331135\n",
      "[100]\tvalid_0's multi_logloss: 0.321719\n",
      "[110]\tvalid_0's multi_logloss: 0.31435\n",
      "[120]\tvalid_0's multi_logloss: 0.308127\n",
      "[130]\tvalid_0's multi_logloss: 0.302548\n",
      "[140]\tvalid_0's multi_logloss: 0.299097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:44:08,996]\u001b[0m Trial 75 finished with value: 0.8764090777082036 and parameters: {'n_estimators': 146, 'learning_rate': 0.0830839357898937, 'num_leaves': 20, 'max_depth': 15, 'min_child_samples': 29}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.877173\n",
      "[20]\tvalid_0's multi_logloss: 0.657514\n",
      "[30]\tvalid_0's multi_logloss: 0.550513\n",
      "[40]\tvalid_0's multi_logloss: 0.48741\n",
      "[50]\tvalid_0's multi_logloss: 0.445456\n",
      "[60]\tvalid_0's multi_logloss: 0.415316\n",
      "[70]\tvalid_0's multi_logloss: 0.392682\n",
      "[80]\tvalid_0's multi_logloss: 0.374975\n",
      "[90]\tvalid_0's multi_logloss: 0.36088\n",
      "[100]\tvalid_0's multi_logloss: 0.34932\n",
      "[110]\tvalid_0's multi_logloss: 0.339746\n",
      "[120]\tvalid_0's multi_logloss: 0.33206\n",
      "[130]\tvalid_0's multi_logloss: 0.325706\n",
      "[140]\tvalid_0's multi_logloss: 0.319476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:44:30,897]\u001b[0m Trial 76 finished with value: 0.8690621186509269 and parameters: {'n_estimators': 146, 'learning_rate': 0.06698535197787757, 'num_leaves': 19, 'max_depth': 13, 'min_child_samples': 26}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.723138\n",
      "[20]\tvalid_0's multi_logloss: 0.54065\n",
      "[30]\tvalid_0's multi_logloss: 0.463062\n",
      "[40]\tvalid_0's multi_logloss: 0.418752\n",
      "[50]\tvalid_0's multi_logloss: 0.389593\n",
      "[60]\tvalid_0's multi_logloss: 0.368076\n",
      "[70]\tvalid_0's multi_logloss: 0.354058\n",
      "[80]\tvalid_0's multi_logloss: 0.341506\n",
      "[90]\tvalid_0's multi_logloss: 0.332194\n",
      "[100]\tvalid_0's multi_logloss: 0.324275\n",
      "[110]\tvalid_0's multi_logloss: 0.31777\n",
      "[120]\tvalid_0's multi_logloss: 0.312357\n",
      "[130]\tvalid_0's multi_logloss: 0.307446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:44:51,644]\u001b[0m Trial 77 finished with value: 0.8596473484780792 and parameters: {'n_estimators': 136, 'learning_rate': 0.10960575872668864, 'num_leaves': 24, 'max_depth': 11, 'min_child_samples': 29}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.803034\n",
      "[20]\tvalid_0's multi_logloss: 0.594587\n",
      "[30]\tvalid_0's multi_logloss: 0.498085\n",
      "[40]\tvalid_0's multi_logloss: 0.443585\n",
      "[50]\tvalid_0's multi_logloss: 0.409143\n",
      "[60]\tvalid_0's multi_logloss: 0.38446\n",
      "[70]\tvalid_0's multi_logloss: 0.366253\n",
      "[80]\tvalid_0's multi_logloss: 0.352255\n",
      "[90]\tvalid_0's multi_logloss: 0.340928\n",
      "[100]\tvalid_0's multi_logloss: 0.332081\n",
      "[110]\tvalid_0's multi_logloss: 0.323719\n",
      "[120]\tvalid_0's multi_logloss: 0.317472\n",
      "[130]\tvalid_0's multi_logloss: 0.311686\n",
      "[140]\tvalid_0's multi_logloss: 0.306527\n",
      "[150]\tvalid_0's multi_logloss: 0.302477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:45:13,607]\u001b[0m Trial 78 finished with value: 0.8691953227798066 and parameters: {'n_estimators': 150, 'learning_rate': 0.0787115858221468, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 35}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.937258\n",
      "[20]\tvalid_0's multi_logloss: 0.707806\n",
      "[30]\tvalid_0's multi_logloss: 0.585783\n",
      "[40]\tvalid_0's multi_logloss: 0.513077\n",
      "[50]\tvalid_0's multi_logloss: 0.465383\n",
      "[60]\tvalid_0's multi_logloss: 0.431199\n",
      "[70]\tvalid_0's multi_logloss: 0.405533\n",
      "[80]\tvalid_0's multi_logloss: 0.386215\n",
      "[90]\tvalid_0's multi_logloss: 0.370598\n",
      "[100]\tvalid_0's multi_logloss: 0.35784\n",
      "[110]\tvalid_0's multi_logloss: 0.346982\n",
      "[120]\tvalid_0's multi_logloss: 0.337865\n",
      "[130]\tvalid_0's multi_logloss: 0.330577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:45:34,919]\u001b[0m Trial 79 finished with value: 0.8745072275492465 and parameters: {'n_estimators': 139, 'learning_rate': 0.05233329393123182, 'num_leaves': 23, 'max_depth': 15, 'min_child_samples': 31}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.923108\n",
      "[20]\tvalid_0's multi_logloss: 0.693134\n",
      "[30]\tvalid_0's multi_logloss: 0.574423\n",
      "[40]\tvalid_0's multi_logloss: 0.503915\n",
      "[50]\tvalid_0's multi_logloss: 0.457804\n",
      "[60]\tvalid_0's multi_logloss: 0.424715\n",
      "[70]\tvalid_0's multi_logloss: 0.399616\n",
      "[80]\tvalid_0's multi_logloss: 0.380967\n",
      "[90]\tvalid_0's multi_logloss: 0.365639\n",
      "[100]\tvalid_0's multi_logloss: 0.353549\n",
      "[110]\tvalid_0's multi_logloss: 0.34299\n",
      "[120]\tvalid_0's multi_logloss: 0.334577\n",
      "[130]\tvalid_0's multi_logloss: 0.327126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:45:56,313]\u001b[0m Trial 80 finished with value: 0.8738910115519514 and parameters: {'n_estimators': 139, 'learning_rate': 0.05455489783650557, 'num_leaves': 23, 'max_depth': 15, 'min_child_samples': 31}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.946443\n",
      "[20]\tvalid_0's multi_logloss: 0.715582\n",
      "[30]\tvalid_0's multi_logloss: 0.593039\n",
      "[40]\tvalid_0's multi_logloss: 0.519646\n",
      "[50]\tvalid_0's multi_logloss: 0.470372\n",
      "[60]\tvalid_0's multi_logloss: 0.434961\n",
      "[70]\tvalid_0's multi_logloss: 0.408808\n",
      "[80]\tvalid_0's multi_logloss: 0.389\n",
      "[90]\tvalid_0's multi_logloss: 0.372843\n",
      "[100]\tvalid_0's multi_logloss: 0.359987\n",
      "[110]\tvalid_0's multi_logloss: 0.349326\n",
      "[120]\tvalid_0's multi_logloss: 0.34041\n",
      "[130]\tvalid_0's multi_logloss: 0.33243\n",
      "[140]\tvalid_0's multi_logloss: 0.326145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:46:17,830]\u001b[0m Trial 81 finished with value: 0.873152777452775 and parameters: {'n_estimators': 140, 'learning_rate': 0.05094117446945261, 'num_leaves': 23, 'max_depth': 15, 'min_child_samples': 31}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.892036\n",
      "[20]\tvalid_0's multi_logloss: 0.667285\n",
      "[30]\tvalid_0's multi_logloss: 0.553268\n",
      "[40]\tvalid_0's multi_logloss: 0.487111\n",
      "[50]\tvalid_0's multi_logloss: 0.443395\n",
      "[60]\tvalid_0's multi_logloss: 0.412772\n",
      "[70]\tvalid_0's multi_logloss: 0.389192\n",
      "[80]\tvalid_0's multi_logloss: 0.371117\n",
      "[90]\tvalid_0's multi_logloss: 0.356829\n",
      "[100]\tvalid_0's multi_logloss: 0.345372\n",
      "[110]\tvalid_0's multi_logloss: 0.33593\n",
      "[120]\tvalid_0's multi_logloss: 0.328267\n",
      "[130]\tvalid_0's multi_logloss: 0.321191\n",
      "[140]\tvalid_0's multi_logloss: 0.315106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:46:40,201]\u001b[0m Trial 82 finished with value: 0.8726045774464308 and parameters: {'n_estimators': 145, 'learning_rate': 0.06104202397630903, 'num_leaves': 21, 'max_depth': 15, 'min_child_samples': 28}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.814226\n",
      "[20]\tvalid_0's multi_logloss: 0.59439\n",
      "[30]\tvalid_0's multi_logloss: 0.491016\n",
      "[40]\tvalid_0's multi_logloss: 0.433805\n",
      "[50]\tvalid_0's multi_logloss: 0.396422\n",
      "[60]\tvalid_0's multi_logloss: 0.371307\n",
      "[70]\tvalid_0's multi_logloss: 0.353043\n",
      "[80]\tvalid_0's multi_logloss: 0.339138\n",
      "[90]\tvalid_0's multi_logloss: 0.328515\n",
      "[100]\tvalid_0's multi_logloss: 0.319579\n",
      "[110]\tvalid_0's multi_logloss: 0.31221\n",
      "[120]\tvalid_0's multi_logloss: 0.305908\n",
      "[130]\tvalid_0's multi_logloss: 0.29996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:47:04,494]\u001b[0m Trial 83 finished with value: 0.8682531425116162 and parameters: {'n_estimators': 133, 'learning_rate': 0.07068195814909087, 'num_leaves': 29, 'max_depth': 16, 'min_child_samples': 30}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.78255\n",
      "[20]\tvalid_0's multi_logloss: 0.572155\n",
      "[30]\tvalid_0's multi_logloss: 0.477369\n",
      "[40]\tvalid_0's multi_logloss: 0.423777\n",
      "[50]\tvalid_0's multi_logloss: 0.390414\n",
      "[60]\tvalid_0's multi_logloss: 0.367511\n",
      "[70]\tvalid_0's multi_logloss: 0.351063\n",
      "[80]\tvalid_0's multi_logloss: 0.337477\n",
      "[90]\tvalid_0's multi_logloss: 0.327132\n",
      "[100]\tvalid_0's multi_logloss: 0.318194\n",
      "[110]\tvalid_0's multi_logloss: 0.310952\n",
      "[120]\tvalid_0's multi_logloss: 0.304947\n",
      "[130]\tvalid_0's multi_logloss: 0.300052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:47:29,423]\u001b[0m Trial 84 finished with value: 0.8673493733385386 and parameters: {'n_estimators': 137, 'learning_rate': 0.08098242790554629, 'num_leaves': 26, 'max_depth': 15, 'min_child_samples': 23}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.856311\n",
      "[20]\tvalid_0's multi_logloss: 0.636268\n",
      "[30]\tvalid_0's multi_logloss: 0.530163\n",
      "[40]\tvalid_0's multi_logloss: 0.46742\n",
      "[50]\tvalid_0's multi_logloss: 0.426276\n",
      "[60]\tvalid_0's multi_logloss: 0.396235\n",
      "[70]\tvalid_0's multi_logloss: 0.374582\n",
      "[80]\tvalid_0's multi_logloss: 0.358611\n",
      "[90]\tvalid_0's multi_logloss: 0.345619\n",
      "[100]\tvalid_0's multi_logloss: 0.335215\n",
      "[110]\tvalid_0's multi_logloss: 0.326568\n",
      "[120]\tvalid_0's multi_logloss: 0.319466\n",
      "[130]\tvalid_0's multi_logloss: 0.31349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:47:49,137]\u001b[0m Trial 85 finished with value: 0.868157032216035 and parameters: {'n_estimators': 139, 'learning_rate': 0.06946164877885114, 'num_leaves': 19, 'max_depth': 17, 'min_child_samples': 32}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.922837\n",
      "[20]\tvalid_0's multi_logloss: 0.692427\n",
      "[30]\tvalid_0's multi_logloss: 0.571767\n",
      "[40]\tvalid_0's multi_logloss: 0.50074\n",
      "[50]\tvalid_0's multi_logloss: 0.453008\n",
      "[60]\tvalid_0's multi_logloss: 0.419547\n",
      "[70]\tvalid_0's multi_logloss: 0.394801\n",
      "[80]\tvalid_0's multi_logloss: 0.375995\n",
      "[90]\tvalid_0's multi_logloss: 0.360964\n",
      "[100]\tvalid_0's multi_logloss: 0.348882\n",
      "[110]\tvalid_0's multi_logloss: 0.338897\n",
      "[120]\tvalid_0's multi_logloss: 0.330616\n",
      "[130]\tvalid_0's multi_logloss: 0.323134\n",
      "[140]\tvalid_0's multi_logloss: 0.317044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:48:13,005]\u001b[0m Trial 86 finished with value: 0.8749285579184218 and parameters: {'n_estimators': 146, 'learning_rate': 0.05396043580958221, 'num_leaves': 23, 'max_depth': 17, 'min_child_samples': 29}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.00687\n",
      "[20]\tvalid_0's multi_logloss: 0.780276\n",
      "[30]\tvalid_0's multi_logloss: 0.652732\n",
      "[40]\tvalid_0's multi_logloss: 0.571822\n",
      "[50]\tvalid_0's multi_logloss: 0.516413\n",
      "[60]\tvalid_0's multi_logloss: 0.477336\n",
      "[70]\tvalid_0's multi_logloss: 0.446479\n",
      "[80]\tvalid_0's multi_logloss: 0.422869\n",
      "[90]\tvalid_0's multi_logloss: 0.403591\n",
      "[100]\tvalid_0's multi_logloss: 0.38841\n",
      "[110]\tvalid_0's multi_logloss: 0.375247\n",
      "[120]\tvalid_0's multi_logloss: 0.364259\n",
      "[130]\tvalid_0's multi_logloss: 0.355064\n",
      "[140]\tvalid_0's multi_logloss: 0.346661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:48:34,658]\u001b[0m Trial 87 finished with value: 0.8657345281938997 and parameters: {'n_estimators': 146, 'learning_rate': 0.04513444893778315, 'num_leaves': 18, 'max_depth': 17, 'min_child_samples': 27}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.858979\n",
      "[20]\tvalid_0's multi_logloss: 0.633189\n",
      "[30]\tvalid_0's multi_logloss: 0.522119\n",
      "[40]\tvalid_0's multi_logloss: 0.458857\n",
      "[50]\tvalid_0's multi_logloss: 0.417675\n",
      "[60]\tvalid_0's multi_logloss: 0.389389\n",
      "[70]\tvalid_0's multi_logloss: 0.368336\n",
      "[80]\tvalid_0's multi_logloss: 0.352242\n",
      "[90]\tvalid_0's multi_logloss: 0.340003\n",
      "[100]\tvalid_0's multi_logloss: 0.330248\n",
      "[110]\tvalid_0's multi_logloss: 0.321773\n",
      "[120]\tvalid_0's multi_logloss: 0.314839\n",
      "[130]\tvalid_0's multi_logloss: 0.308774\n",
      "[140]\tvalid_0's multi_logloss: 0.303604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:48:56,381]\u001b[0m Trial 88 finished with value: 0.8749810730892102 and parameters: {'n_estimators': 142, 'learning_rate': 0.06399646780018722, 'num_leaves': 25, 'max_depth': 17, 'min_child_samples': 37}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.875731\n",
      "[20]\tvalid_0's multi_logloss: 0.64863\n",
      "[30]\tvalid_0's multi_logloss: 0.535114\n",
      "[40]\tvalid_0's multi_logloss: 0.469717\n",
      "[50]\tvalid_0's multi_logloss: 0.427128\n",
      "[60]\tvalid_0's multi_logloss: 0.397664\n",
      "[70]\tvalid_0's multi_logloss: 0.375568\n",
      "[80]\tvalid_0's multi_logloss: 0.359043\n",
      "[90]\tvalid_0's multi_logloss: 0.346157\n",
      "[100]\tvalid_0's multi_logloss: 0.33538\n",
      "[110]\tvalid_0's multi_logloss: 0.326668\n",
      "[120]\tvalid_0's multi_logloss: 0.319214\n",
      "[130]\tvalid_0's multi_logloss: 0.313056\n",
      "[140]\tvalid_0's multi_logloss: 0.308121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:49:18,014]\u001b[0m Trial 89 finished with value: 0.8721887295691568 and parameters: {'n_estimators': 147, 'learning_rate': 0.06142978915466631, 'num_leaves': 25, 'max_depth': 16, 'min_child_samples': 37}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.940933\n",
      "[20]\tvalid_0's multi_logloss: 0.712752\n",
      "[30]\tvalid_0's multi_logloss: 0.592234\n",
      "[40]\tvalid_0's multi_logloss: 0.521503\n",
      "[50]\tvalid_0's multi_logloss: 0.474184\n",
      "[60]\tvalid_0's multi_logloss: 0.440717\n",
      "[70]\tvalid_0's multi_logloss: 0.415059\n",
      "[80]\tvalid_0's multi_logloss: 0.395118\n",
      "[90]\tvalid_0's multi_logloss: 0.379549\n",
      "[100]\tvalid_0's multi_logloss: 0.36619\n",
      "[110]\tvalid_0's multi_logloss: 0.355527\n",
      "[120]\tvalid_0's multi_logloss: 0.346298\n",
      "[130]\tvalid_0's multi_logloss: 0.338465\n",
      "[140]\tvalid_0's multi_logloss: 0.331359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:49:40,889]\u001b[0m Trial 90 finished with value: 0.8594054380955374 and parameters: {'n_estimators': 144, 'learning_rate': 0.053863001404900135, 'num_leaves': 21, 'max_depth': 14, 'min_child_samples': 25}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.76734\n",
      "[20]\tvalid_0's multi_logloss: 0.559071\n",
      "[30]\tvalid_0's multi_logloss: 0.465146\n",
      "[40]\tvalid_0's multi_logloss: 0.413332\n",
      "[50]\tvalid_0's multi_logloss: 0.379141\n",
      "[60]\tvalid_0's multi_logloss: 0.35585\n",
      "[70]\tvalid_0's multi_logloss: 0.338783\n",
      "[80]\tvalid_0's multi_logloss: 0.326395\n",
      "[90]\tvalid_0's multi_logloss: 0.316555\n",
      "[100]\tvalid_0's multi_logloss: 0.308957\n",
      "[110]\tvalid_0's multi_logloss: 0.302647\n",
      "[120]\tvalid_0's multi_logloss: 0.297376\n",
      "[130]\tvalid_0's multi_logloss: 0.292999\n",
      "[140]\tvalid_0's multi_logloss: 0.28933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:50:02,356]\u001b[0m Trial 91 finished with value: 0.8785013909150619 and parameters: {'n_estimators': 142, 'learning_rate': 0.08472974365943982, 'num_leaves': 23, 'max_depth': 18, 'min_child_samples': 34}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.775185\n",
      "[20]\tvalid_0's multi_logloss: 0.566901\n",
      "[30]\tvalid_0's multi_logloss: 0.471146\n",
      "[40]\tvalid_0's multi_logloss: 0.418097\n",
      "[50]\tvalid_0's multi_logloss: 0.383644\n",
      "[60]\tvalid_0's multi_logloss: 0.36021\n",
      "[70]\tvalid_0's multi_logloss: 0.343662\n",
      "[80]\tvalid_0's multi_logloss: 0.330942\n",
      "[90]\tvalid_0's multi_logloss: 0.320664\n",
      "[100]\tvalid_0's multi_logloss: 0.312007\n",
      "[110]\tvalid_0's multi_logloss: 0.305007\n",
      "[120]\tvalid_0's multi_logloss: 0.299591\n",
      "[130]\tvalid_0's multi_logloss: 0.295493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:50:23,271]\u001b[0m Trial 92 finished with value: 0.8714658894856704 and parameters: {'n_estimators': 136, 'learning_rate': 0.08321924659626538, 'num_leaves': 23, 'max_depth': 17, 'min_child_samples': 34}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.954954\n",
      "[20]\tvalid_0's multi_logloss: 0.72236\n",
      "[30]\tvalid_0's multi_logloss: 0.595613\n",
      "[40]\tvalid_0's multi_logloss: 0.519309\n",
      "[50]\tvalid_0's multi_logloss: 0.469595\n",
      "[60]\tvalid_0's multi_logloss: 0.433415\n",
      "[70]\tvalid_0's multi_logloss: 0.407026\n",
      "[80]\tvalid_0's multi_logloss: 0.38654\n",
      "[90]\tvalid_0's multi_logloss: 0.370482\n",
      "[100]\tvalid_0's multi_logloss: 0.357148\n",
      "[110]\tvalid_0's multi_logloss: 0.346382\n",
      "[120]\tvalid_0's multi_logloss: 0.337785\n",
      "[130]\tvalid_0's multi_logloss: 0.329818\n",
      "[140]\tvalid_0's multi_logloss: 0.322951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:50:46,765]\u001b[0m Trial 93 finished with value: 0.873949786385875 and parameters: {'n_estimators': 141, 'learning_rate': 0.04848902476711339, 'num_leaves': 25, 'max_depth': 17, 'min_child_samples': 29}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.859461\n",
      "[20]\tvalid_0's multi_logloss: 0.63597\n",
      "[30]\tvalid_0's multi_logloss: 0.526422\n",
      "[40]\tvalid_0's multi_logloss: 0.462924\n",
      "[50]\tvalid_0's multi_logloss: 0.420647\n",
      "[60]\tvalid_0's multi_logloss: 0.390785\n",
      "[70]\tvalid_0's multi_logloss: 0.369473\n",
      "[80]\tvalid_0's multi_logloss: 0.35321\n",
      "[90]\tvalid_0's multi_logloss: 0.341148\n",
      "[100]\tvalid_0's multi_logloss: 0.330911\n",
      "[110]\tvalid_0's multi_logloss: 0.322634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:51:04,367]\u001b[0m Trial 94 finished with value: 0.8674927485152842 and parameters: {'n_estimators': 117, 'learning_rate': 0.06657788766925554, 'num_leaves': 21, 'max_depth': 18, 'min_child_samples': 34}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.709236\n",
      "[20]\tvalid_0's multi_logloss: 0.512768\n",
      "[30]\tvalid_0's multi_logloss: 0.430012\n",
      "[40]\tvalid_0's multi_logloss: 0.385635\n",
      "[50]\tvalid_0's multi_logloss: 0.357656\n",
      "[60]\tvalid_0's multi_logloss: 0.339246\n",
      "[70]\tvalid_0's multi_logloss: 0.32531\n",
      "[80]\tvalid_0's multi_logloss: 0.314342\n",
      "[90]\tvalid_0's multi_logloss: 0.305756\n",
      "[100]\tvalid_0's multi_logloss: 0.299072\n",
      "[110]\tvalid_0's multi_logloss: 0.29422\n",
      "[120]\tvalid_0's multi_logloss: 0.289689\n",
      "[130]\tvalid_0's multi_logloss: 0.286033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:51:25,965]\u001b[0m Trial 95 finished with value: 0.8719314285199735 and parameters: {'n_estimators': 132, 'learning_rate': 0.09855811933120515, 'num_leaves': 27, 'max_depth': 16, 'min_child_samples': 32}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 1.03165\n",
      "[20]\tvalid_0's multi_logloss: 0.803055\n",
      "[30]\tvalid_0's multi_logloss: 0.671584\n",
      "[40]\tvalid_0's multi_logloss: 0.587645\n",
      "[50]\tvalid_0's multi_logloss: 0.530042\n",
      "[60]\tvalid_0's multi_logloss: 0.488415\n",
      "[70]\tvalid_0's multi_logloss: 0.456856\n",
      "[80]\tvalid_0's multi_logloss: 0.431247\n",
      "[90]\tvalid_0's multi_logloss: 0.411192\n",
      "[100]\tvalid_0's multi_logloss: 0.395238\n",
      "[110]\tvalid_0's multi_logloss: 0.381348\n",
      "[120]\tvalid_0's multi_logloss: 0.369857\n",
      "[130]\tvalid_0's multi_logloss: 0.360189\n",
      "[140]\tvalid_0's multi_logloss: 0.351531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:51:45,307]\u001b[0m Trial 96 finished with value: 0.8669564059549515 and parameters: {'n_estimators': 145, 'learning_rate': 0.04128003020271523, 'num_leaves': 19, 'max_depth': 18, 'min_child_samples': 37}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.917733\n",
      "[20]\tvalid_0's multi_logloss: 0.690035\n",
      "[30]\tvalid_0's multi_logloss: 0.571226\n",
      "[40]\tvalid_0's multi_logloss: 0.501719\n",
      "[50]\tvalid_0's multi_logloss: 0.456056\n",
      "[60]\tvalid_0's multi_logloss: 0.423252\n",
      "[70]\tvalid_0's multi_logloss: 0.398498\n",
      "[80]\tvalid_0's multi_logloss: 0.379705\n",
      "[90]\tvalid_0's multi_logloss: 0.36469\n",
      "[100]\tvalid_0's multi_logloss: 0.352171\n",
      "[110]\tvalid_0's multi_logloss: 0.341991\n",
      "[120]\tvalid_0's multi_logloss: 0.333197\n",
      "[130]\tvalid_0's multi_logloss: 0.326077\n",
      "[140]\tvalid_0's multi_logloss: 0.319895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:52:08,396]\u001b[0m Trial 97 finished with value: 0.8694493406360806 and parameters: {'n_estimators': 148, 'learning_rate': 0.056029545066570284, 'num_leaves': 22, 'max_depth': 15, 'min_child_samples': 30}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.860642\n",
      "[20]\tvalid_0's multi_logloss: 0.635326\n",
      "[30]\tvalid_0's multi_logloss: 0.524539\n",
      "[40]\tvalid_0's multi_logloss: 0.461383\n",
      "[50]\tvalid_0's multi_logloss: 0.420064\n",
      "[60]\tvalid_0's multi_logloss: 0.391441\n",
      "[70]\tvalid_0's multi_logloss: 0.369858\n",
      "[80]\tvalid_0's multi_logloss: 0.35357\n",
      "[90]\tvalid_0's multi_logloss: 0.34104\n",
      "[100]\tvalid_0's multi_logloss: 0.330694\n",
      "[110]\tvalid_0's multi_logloss: 0.322214\n",
      "[120]\tvalid_0's multi_logloss: 0.315432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:52:28,048]\u001b[0m Trial 98 finished with value: 0.8738211286418363 and parameters: {'n_estimators': 127, 'learning_rate': 0.06447971968470011, 'num_leaves': 24, 'max_depth': 17, 'min_child_samples': 36}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\AppData\\Local\\Temp\\ipykernel_8204\\1026598587.py:35: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tvalid_0's multi_logloss: 0.78723\n",
      "[20]\tvalid_0's multi_logloss: 0.57144\n",
      "[30]\tvalid_0's multi_logloss: 0.473427\n",
      "[40]\tvalid_0's multi_logloss: 0.418905\n",
      "[50]\tvalid_0's multi_logloss: 0.385333\n",
      "[60]\tvalid_0's multi_logloss: 0.361461\n",
      "[70]\tvalid_0's multi_logloss: 0.344364\n",
      "[80]\tvalid_0's multi_logloss: 0.331316\n",
      "[90]\tvalid_0's multi_logloss: 0.321064\n",
      "[100]\tvalid_0's multi_logloss: 0.313047\n",
      "[110]\tvalid_0's multi_logloss: 0.306158\n",
      "[120]\tvalid_0's multi_logloss: 0.300772\n",
      "[130]\tvalid_0's multi_logloss: 0.295309\n",
      "[140]\tvalid_0's multi_logloss: 0.291326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-19 03:52:53,564]\u001b[0m Trial 99 finished with value: 0.8679434477712842 and parameters: {'n_estimators': 142, 'learning_rate': 0.07593790342426376, 'num_leaves': 30, 'max_depth': 16, 'min_child_samples': 28}. Best is trial 71 with value: 0.8810595533290522.\u001b[0m\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\silvi\\anaconda3\\envs\\test\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 149, 'learning_rate': 0.10596416395908402, 'num_leaves': 19, 'max_depth': 13, 'min_child_samples': 35}\n",
      "[10]\tvalid_0's multi_logloss: 0.726279\n",
      "[20]\tvalid_0's multi_logloss: 0.540229\n",
      "[30]\tvalid_0's multi_logloss: 0.4573\n",
      "[40]\tvalid_0's multi_logloss: 0.411331\n",
      "[50]\tvalid_0's multi_logloss: 0.381611\n",
      "[60]\tvalid_0's multi_logloss: 0.360758\n",
      "[70]\tvalid_0's multi_logloss: 0.345459\n",
      "[80]\tvalid_0's multi_logloss: 0.333706\n",
      "[90]\tvalid_0's multi_logloss: 0.323556\n",
      "[100]\tvalid_0's multi_logloss: 0.31559\n",
      "[110]\tvalid_0's multi_logloss: 0.309176\n",
      "[120]\tvalid_0's multi_logloss: 0.303208\n",
      "[130]\tvalid_0's multi_logloss: 0.298744\n",
      "[140]\tvalid_0's multi_logloss: 0.29493\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Load the train data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "\n",
    "# Load the test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Define the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the CountVectorizer to the training data\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['label']\n",
    "\n",
    "# Convert the features to float64\n",
    "X_train = csr_matrix(X_train, dtype='float64')\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'multiclass',\n",
    "        'metric': 'multi_logloss',\n",
    "        'num_class': 8,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.5),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 10, 60),\n",
    "        'max_depth': trial.suggest_int('max_depth', -1, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric='multi_logloss')\n",
    "\n",
    "    y_pred = model.predict(X_valid)\n",
    "    f1 = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "    return f1\n",
    "\n",
    "# Define the Optuna study object\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Optimize the objective function using Optuna\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(study.best_params)\n",
    "\n",
    "# Train the LightGBM model on the training data using the best hyperparameters\n",
    "params = study.best_params\n",
    "params['objective'] = 'multiclass'\n",
    "params['metric'] = 'multi_logloss'\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=10, verbose=10, eval_metric='multi_logloss')\n",
    "\n",
    "# Transform the test data using the CountVectorizer\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Convert the features to float64\n",
    "X_test = csr_matrix(X_test, dtype='float64')\n",
    "\n",
    "# Make predictions on the test data using the trained LightGBM model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to the submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'label': y_pred})\n",
    "submission_df.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14707855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
